# -*- coding: utf-8 -*-
"""Chapter8:【提出課題: 解答用】会話履歴を踏まえて回答するLLMシステムの開発

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1feiT21OwYH09ulguL4EIU5T6Etvat-0A

# Lesson6: OpenAI API
# Chapter8:【提出課題: 解答用】会話履歴を踏まえて回答するLLMシステムの開発

事前準備を行った上で、提出課題に取り組みましょう。

**【提出方法】**  
Slackでメンターをメンションの上、当シート右上の「共有」からリンクをコピーし、提出してください。

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""各種パッケージのインストール"""

!pip install openai==1.47.0 httpx==0.27.2

"""## 提出課題
以下の条件に従って、会話履歴を踏まえてLLMに回答させるプログラムを作ってください。

*   「健康に関する専門家」としてLLMに振る舞わせる。
*   while文とinput関数を使い、連続でLLMへのユーザー入力を受け取れるようにする。
*   ユーザーが「quit」と入力したら、ループ処理を終わらせられるようにする。
*   LLMリクエストの際に指定するパラメータについて、「temperature」の値を「0.7」に設定する。
*   LLMから回答取得後、毎回入出力に消費したトークン数を合計トークン数に加算し、表示する（「tiktoken」パッケージの使用は任意とする）。


"""

!pip install tiktoken==0.8.0

from openai import OpenAI
import tiktoken

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
enc = tiktoken.get_encoding("cl100k_base")

total_tokens = 0

conversation_history = [
    {"role": "system", "content": "あなたは健康に関する専門家です。"}
]
while True:
    input_data = input("健康に関する質問（quitで終了）: ")
    if input_data == "quit":
        break

    conversation_history.append({"role": "user", "content": input_data})
    input_tokens = len(enc.encode(input_data))
    total_tokens += input_tokens

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=conversation_history,
        temperature=0.7
    )

    result = completion.choices[0].message.content
    conversation_history.append({"role": "assistant", "content": result})

    print(result)
    print("=======================")

    response_tokens = len(enc.encode(result))
    total_tokens += response_tokens
    print(f"合計トークン数: {total_tokens}")