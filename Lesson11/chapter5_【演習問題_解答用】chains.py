# -*- coding: utf-8 -*-
"""Chapter5:【演習問題: 解答用】Chains

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cawllSuiJTpr5J3fb-YAojZ9PaW7wcq8

# Lesson11: LangChainの主要モジュール2【Chains】
# Chapter5:【演習問題: 解答用】Chains

事前準備を行った上で、3つの演習問題に取り組みましょう。

各問題の「回答例/正解」と「解説」はデフォルトで非表示としていますが、  
非表示セルをクリックすれば確認できます。

まずは「回答例/正解」と「解説」を見ずにトライしてみましょう。

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""各種パッケージのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 langchain-openai==0.2.2 httpx==0.27.2

"""## 演習問題

### 【問題1】
食材と料理ジャンル（「中華」など）を入力とし、その料理ジャンルにおける、その食材を使ったユニークな料理アイデアをLLMで3つ生成して、リスト形式で受け取るためのプログラムをLLMChainを使って作りましょう。
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.output_parsers import CommaSeparatedListOutputParser

# 入力する地名を穴埋めとしたPromptTemplateとChat modelsのLLMを用意
template = """
その料理ジャンルにおける、その食材を使ったユニークな料理アイデアを3つ教えてください。
{format_instruction}
ジャンル：{genre}
食材：{foodstuffs}
料理アイデア：
"""

# 出力形式のインスタンス生成
output_parser = CommaSeparatedListOutputParser()

# 生成したフォーマット命令をプロンプトに埋め込む
prompt = PromptTemplate(
    input_variables=["place", "foodstuffs"],
    template=template,
    partial_variables={"format_instruction": output_parser.get_format_instructions()}
)

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
# 引数にPromptTemplateとLLMを渡し、LLMChainのインスタンスを作成
chain = LLMChain(prompt=prompt, llm=llm, verbose=True)

# LLMChainの「run()」メソッドを呼び出し、
# 引数にPromptTemplateで設定した変数名と、その変数名を置換する値を渡す
result = chain.run(genre="和食", foodstuffs="魚")
print(result)

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.output_parsers import CommaSeparatedListOutputParser

template = """
以下のジャンルにおける、食材を使ったユニークな料理アイデアを3つ挙げてください。

{format_instruction}

食材：{ingredients}
料理ジャンル：{genre}
料理アイデア：
"""

output_parser = CommaSeparatedListOutputParser()

format_instruction = output_parser.get_format_instructions()

prompt = PromptTemplate(
    template=template,
    input_variables=["ingredients", "genre"],
    partial_variables={"format_instruction": format_instruction}
)

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser, verbose=True)
result = chain.run(ingredients="豆腐", genre="イタリアン")
result

"""#### 【解説】

LLMChainを使うことで、「呼び出すモデル」と「プロンプト」、「欲しい出力形式と回答取得後の型変換処理」を連鎖的に実行できます。

リスト形式で回答を受け取るため、CommaSeparatedListOutputParserでフォーマット命令を作り、プロンプトに埋め込んでいます。

今回は2つの入力値を受け取ってプロンプトに埋め込むため、プロンプトのテンプレートには「ingredients」と「genre」の2つの変数を用意しています。

### 【問題2】
業界名を入力とし、その業界におけるビジネスアイデアを3つLLMで生成した後に、生成した各ビジネスアイデアについてマーケティング戦略をLLMで生成するプログラムをSimpleSequentialChainを使って作りましょう。
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.chains import SimpleSequentialChain

# ChatOpenAIのインスタンス（Chat Completions APIにアクセスするためのPythonオブジェクト）を用意
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# テンプレートの中で、後ほど入力値として渡される予定の小説のテーマ「theme」を変数として定義
first_template = """
あなたはビジネスを企画する専門家です。以下の業界におけるビジネスアイデアを3つ提案してください。
業界：{industry}
ビジネスアイデア：
"""

# 呼び出すモデルとPromptTemplateをLLMChainに渡す
first_prompt = PromptTemplate(
    input_variables=["industry"],
    template=first_template
)

# 変数「first_chain」は、LLMChainクラスのインスタンス
# 後ほどChainの処理を走らせることで、内部的にプロンプト生成とLLMへのリクエスト処理が行われる
first_chain = LLMChain(llm=llm, prompt=first_prompt)


second_template = """
あなたはマーケティングの専門家です。ビジネスアイデアについてマーケティング戦略を提案してください。
ビジネスアイデア：{idea}
マーケティング戦略：
"""

# 1つ目のChainで生成した小説の導入テキストを入力として受け取り、レビューを行うChainを、同じくLLMChainで作る
# 後ほど、テンプレートの中の「introduction」に、1つ目のChainによる回答結果が挿入される
second_prompt = PromptTemplate(
    input_variables=["idea"],
    template=second_template
)

# 変数「first_chain」は、LLMChainクラスのインスタンス
second_chain = LLMChain(llm=llm, prompt=second_prompt)

# SimpleSequentialChainクラスのchains引数に、1つ目のChainと2つ目のChainをリストで渡し、2つのChainを繋ぎ合わせたChainを作る
chain = SimpleSequentialChain(chains=[first_chain, second_chain])

# Chainに小説のテーマを与えて実行
result = chain("金融")

# Chainを実行した回答結果は辞書型で返ってくる
# 「output」というキーに回答結果のテキストが格納されているため、取り出して表示
print(result["output"])

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.chains import SimpleSequentialChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

first_template = """
あなたはビジネスアイデアを生成する専門家です。
以下の業界におけるビジネスアイデアを3つ生成してください。

業界：{industry}
ビジネスアイデア：
"""

first_prompt = PromptTemplate(
    template=first_template,
    input_variables=["industry"],
)

first_chain = LLMChain(llm=llm, prompt=first_prompt)


second_template = """
あなたはビジネスアイデアからマーケティング戦略を生成する専門家です。
以下の3つのビジネスアイデアについて、それぞれマーケティング戦略を立案してください。

ビジネスアイデア：{business_idea}
マーケティング戦略：
"""

second_prompt = PromptTemplate(
    input_variables=["business_idea"],
    template=second_template,
)

second_chain = LLMChain(llm=llm, prompt=second_prompt)


chain = SimpleSequentialChain(chains=[first_chain, second_chain])

result = chain("IT")
result["output"]

"""#### 【解説】

LLMの回答結果を、次のLLMへのリクエストに含めることで「段階的な推論」を行い、回答精度を向上させる手法はよく使います。

今回は、入力した業界におけるビジネスアイデアをLLMで生成し、生成したビジネスアイデアを次のLLMへの入力として渡す仕様です。今回は入出力データが1つであるため、連鎖的に処理を実行するために「SimpleSequentialChain」を使っています。

### 【問題3】
あなたは会社の経営者です。現在の年商と、3年以内の目標年商を入力として受け取り、現在の年商の規模感の会社が目標年商を目指すにあたって多くの企業が汎用的に抱えている課題をLLMで生成した後に、その課題を解決して目標年商を達成するための具体的な方法を再度LLMで生成するプログラムをSequentialChainを使って作りましょう。
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.chains import SequentialChain

# モデルのインスタンスを用意
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# テーマを与えてLLMに小説の導入を書いてもらうためのChainを、LLMChainで作
first_template = """
あなたは経営コンサルタントです。現在の年商の規模感の会社が目標年商を目指すにあたって多くの企業が汎用的に抱えている課題を教えてください。
現在の年商：{current_annual_sales}
3年以内の目標年商：{target_annual_sales}
汎用的に抱えている課題：
"""

# SimpleSequentialChainではinput_variablesが「theme」の1つだけでしたが、今回は「tone」を加えて2つ
first_prompt = PromptTemplate(
    input_variables=["current_annual_sales", "target_annual_sales"],
    template=first_template,
)

# 今回は1つ目のChainの回答結果を2つ目のChainの入力として渡すため、明示的にLLMChainクラスの引数に「output_key="introduction"」を追加
# これにより、1つ目のChainの回答結果が「introduction」という変数名で2つ目のChainに渡される
first_chain = LLMChain(
    llm=llm,
    prompt=first_prompt,
    output_key="issue"
)

# 2つ目のChainを作る
second_template = """
あなたは経営コンサルタントです。汎用的に抱えている課題を解決して目標年商を達成するための具体的な方法を教えてください。
汎用的に抱えている課題：{issue}
具体的な方法：
"""

# 1つ目のChainの回答結果が「introduction」という変数名で渡ってくるため、挿入できるよう変数をセット
second_prompt = PromptTemplate(
    input_variables=["issue"],
    template=second_template
)

# 2つ目のLLMChainにも、「output_key」引数を設定
# これにより、2つのChainを連結させたSequentialChainの回答結果（レスポンスデータ）の中から、
# output_key引数に設定した「review」をキーに指定することで回答テキストを取り出せる
second_chain = LLMChain(
    llm=llm,
    prompt=second_prompt,
    output_key="answer"
)

# 連結する2つのChainが用意できたら、SequentialChainを作る
# 作成した2つのChainを渡し、1つ目のChainで入力を受け取る変数名をinput_variablesで、また1つ目のChainの戻り値と、2つ目のChainの戻り値の変数名をoutput_variablesで設定
chain = SequentialChain(
    chains=[first_chain, second_chain],
    input_variables=["current_annual_sales", "target_annual_sales"],
    output_variables=["issue", "answer"],
)

# SequentialChainを実行
# 1つ目のChainの入力に使った「theme」と「tone」、また1つ目のChainの出力である「introduction」と、2つ目のChainの出力である「review」が辞書型で格納されています。
result = chain({"current_annual_sales":"1億", "target_annual_sales": "3億"})

print(result)

# LLMが生成した小説に対してのレビュー内容を取り出せる
print(result["answer"])

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.chains import SequentialChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

first_template = """
あなたは優秀な経営コンサルタントです。
現在の年商から、3年以内に目標年商を達成するために多くの企業が汎用的に抱えている課題を教えてください。

現在の年商：{current_sales}
目標年商：{target_sales}
課題：
"""

first_prompt = PromptTemplate(
    input_variables=["current_sales", "target_sales"],
    template=first_template,
)

first_chain = LLMChain(
    llm=llm,
    prompt=first_prompt,
    output_key="problem"
)


second_template = """
あなたは優秀な経営コンサルタントです。
以下の課題を解決して目標年商を達成するための具体的な方法を生成してください。

現在の年商：{current_sales}
目標年商：{target_sales}
課題：{problem}
方法：
"""

second_prompt = PromptTemplate(
    input_variables=["problem", "current_sales", "target_sales"],
    template=second_template,
)

second_chain = LLMChain(
    llm=llm,
    prompt=second_prompt,
    output_key="method"
)


chain = SequentialChain(
    chains=[first_chain, second_chain],
    input_variables=["current_sales", "target_sales"],
    output_variables=["problem", "method"],
)

result = chain({"current_sales":"3億円", "target_sales": "5億円"})
result

"""#### 【解説】

「現在の年商」と「3年以内の目標年商」という2つの入力を受け取るため、Chainsの複数の入出力に対応する「SequentialChain」を使います。

1つ目のChainsでは「current_sales」と「target_sales」の2つを入力として受け取り、呼び出しの際も辞書形式で「3億円」「5億円」とそれぞれ指定しています。

1つ目のChainsの回答結果を「problem」で受け取り、また1つ目の入力として渡した「current_sales」と「target_sales」を2つ目のChainsでも使っています。

2つ目のChainsでは「output_key」を「method」としているため、最終的にLLMから返された辞書データの「method」キーの中に、回答結果が格納されています。

### 【問題4】
input関数で「虐殺、抹殺、虐待」というユーザー入力を受け取り、入力値がOpenAIの利用ポリシーに反している場合に例外を発生させ、例外メッセージを表示するプログラムを作りましょう。

※場合によってはOpenAIの利用ポリシーに反していないと判断され、例外が発生しない可能性があります。
"""

# 「OpenAIModerationChain」クラスを読み込み、インスタンスを作る
from langchain.chains import OpenAIModerationChain
chain = OpenAIModerationChain(error=True)

try:
  value = input("入力してください")
  chain.run(value)
except ValueError as e:
    print("OpenAIの利用ポリシーに反する、不適切な入力が与えられました。")
    print(f"例外詳細: {e}")

"""#### 【解答例】"""

from langchain.chains import OpenAIModerationChain

chain = OpenAIModerationChain(error=True)

try:
    input_data = input("何らかの文章を入力してください: ")
    chain.run(input_data)
except ValueError as e:
    print("OpenAIの利用ポリシーに反する、不適切な入力が与えられました。")
    print(f"例外詳細: {e}")

"""#### 【解説】

OpenAIの利用ポリシーに反していないかをチェックするためのChainsが、OpenAIModerationChainです。

「error=True」と引数を指定することで、OpenAIの利用ポリシーに反する入力が与えられた場合に明示的に例外を発生させられます。

例外が発生した場合、「try - except」で例外をキャッチし、適切な例外メッセージを表示することが重要です。

### 【問題5】
「Chapter4: その他のChains」で解説した「LLMRouterChain」を使い、社内チャットボットを想定して、「営業」「総務」「経理」「人事」「マーケティング」のそれぞれで用意されたテンプレートのうち、ユーザーの入力内容に応じて使用するプロンプトのテンプレートを分岐させる処理を実装してください。

#### 【ヒント】

「Chapter4: その他のChains」で解説した「LLMRouterChain」のコードと、ほとんど同じもので問題ありません。各種類ごとのテンプレートと、「prompt_infos」の説明一覧を用意されたものに置き換え、LLMへの質問を変えるだけで実装できます。

#### 【解答】
"""

# 各テーマごとのプロンプトのテンプレート
sales_template = """
あなたは営業活動を支援する専門家です。
見込み顧客の獲得、提案書の作成、営業トーク、クロージングの戦略についてアドバイスを行います。
また顧客対応の改善や新規商談のアイデアについてもサポートします。

質問：{input}
"""

admin_template = """
あなたは総務業務に詳しいアドバイザーです。
社内ルールや設備管理、福利厚生、会社行事の運営について質問に答えます。
また従業員が働きやすい環境を整えるためのアドバイスを提供します。

質問：{input}
"""

finance_template = """
あなたは経理業務の専門家です。
経費精算、税務処理、予算編成、月次報告に関する質問に答えます。
また社内での財務手続きやガイドラインの確認にも対応します。

質問：{input}
"""

hr_template = """
あなたは人事部門の支援を行うエキスパートです。
採用活動、従業員研修、評価制度、労務管理に関する質問に答えます。
また従業員がキャリアを発展させるためのサポートも行います。

質問：{input}
"""

marketing_template = """
あなたはマーケティング活動をサポートする専門家です。
プロモーション戦略、SNS活用、データ分析、広告キャンペーンの計画についてアドバイスを提供します。
また市場動向やターゲット分析についても詳しく説明します。

質問：{input}
"""

# 各テーマごとのプロンプトのテンプレートを辞書型で用意
prompt_infos = [
    {
        "name": "sales",
        "description": "営業活動を支援する専門家です",
        "prompt_template": sales_template
    },
    {
        "name": "admin",
        "description": "総務業務に詳しい専門家です",
        "prompt_template": admin_template
    },
    {
        "name": "finance",
        "description": "経理業務の専門家です",
        "prompt_template": finance_template
    },
    {
        "name": "hr",
        "description": "人事部門の支援を行う専門家です",
        "prompt_template": hr_template
    },
    {
        "name": "marketing",
        "description": "マーケティング活動をサポートする専門家です",
        "prompt_template": marketing_template
    }
]

# -----------------------------------------------------
# 2. 呼び出すモデルのインスタンスを作る
# -----------------------------------------------------
# 汎用的に使う各種モジュールを読み込む
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain

# 各Chainで呼び出すモデルのインスタンスを作る
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# -----------------------------------------------------
# 3. 候補となるChainの一覧を、名前付きで辞書に登録する
# -----------------------------------------------------
# 入力内容に応じてLLMにどのChainを実行するか判断してもらうため、
# 最終的に「候補となるChainの一覧」をMultiPromptChainと呼ばれるChainに渡す必要がある
destination_chains = {}

# 用意した各テンプレートごとにChainを作り、名前をキーにして辞書に登録
# ループ処理の中身はシンプルで、先ほど作成した「名前・説明・テンプレート」が
# セットとなった要素に対してループ処理を行い、各テンプレートごとにChainを作って名前をキーにして辞書に登録
for prompt_info in prompt_infos:
    name = prompt_info["name"]
    prompt_template = prompt_info["prompt_template"]
    prompt = PromptTemplate(template=prompt_template)
    chain = LLMChain(llm=llm, prompt=prompt)
    destination_chains[name] = chain

destination_chains

# -----------------------------------------------------
# 4. 最初の入力を受け取るChainを用意する
# -----------------------------------------------------
# 候補となるChainの一覧と同じく、最初の入力を受け取るChainも、最終的にMultiPromptChainに渡す
default_prompt = PromptTemplate(input_variables=["input"], template="{input}")
# 最初の入力を「input」として受け取るシンプルなLLMChainを用意します
default_chain = LLMChain(llm=llm, prompt=default_prompt)

# -----------------------------------------------------
# 5. LLMRouterChainのテンプレートと、各Chainの名前と説明をセットにしたテキストを結合して
# PromptTemplateを作り、それを活用してLLMRouterChainを作る
# -----------------------------------------------------
# LLMRouterChainを作るためには、まず入力内容に応じてどのChainを使うか判断するためのプロンプトを作る
# 「あらかじめ用意されているテンプレート」と「各Chainの名前と説明をセットにしたテキスト」を結合する
# まずは「各Chainの名前と説明をセットにしたテキスト」を作る
destinations = []

#先ほど用意した各Chainの名前・説明・テンプレートのセットが格納されている「prompt_infos」をループ
for p in prompt_infos:
    # 各要素から名前（name）と説明（description）を取り出し、順にリストに格納
    destinations.append(f"{p['name']}: {p['description']}")

# 「あらかじめ用意されているテンプレート」と結合するため、リストを文字列に変換
# 「join()」メソッドを使い、各要素を改行区切りで文字列に変換
destinations_str = "\n".join(destinations)

print(destinations_str)

# 「あらかじめ用意されているテンプレート」と「各Chainの名前と説明をセットにしたテキスト」を結合し、
# 入力内容に応じてどのChainを実行するか判断するために使うプロンプトテンプレートを作成。
# MULTI_PROMPT_ROUTER_TEMPLATEの「format()」メソッドのdestinations引数に、
# 「各Chainの名前と説明のセット」である「destinations_str」を指定することで、プロンプトテンプレートが作成される
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)

print(f"------------------- router_template ------------------\n{router_template}")

from langchain.chains.router.llm_router import RouterOutputParser

# templateには、先ほど作成した一部穴埋めのあるテンプレート「router_template」を設定し、入力変数としてinput_variablesに「input」を設定
# またoutput_parserには、RouterOutputParserのインスタンスを設定
# これは入力内容に応じてどのChainを実行するかLLMに判断してもらうための、次のChainへの出力形式を指定するものです。
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)

# LLMRouterChainクラスの「from_llm()」メソッドに、呼び出すモデル「llm」と、
# 入力内容に応じてどのChainを実行するかLLMに判断させるためのプロンプト「router_prompt」を渡す
from langchain.chains.router.llm_router import LLMRouterChain
router_chain = LLMRouterChain.from_llm(llm, router_prompt)

# -----------------------------------------------------
# 6. 最初の入力を受け取るのに使うChainと、
# 入力内容に応じてどのChainを実行するかを判断してもらうためのLLMRouterChainと、
# 候補となるChainの一覧を元にMultiPromptChainを作る
# -----------------------------------------------------
# ここまでで、以下3つの要素を用意できた。
# 1.最初の入力を受け取るのに使うChain（default_chain）
# 2.入力内容に応じてどのChainを実行するかを判断してもらうためのLLMRouterChain（router_chain）
# 3.候補となるChainの一覧（destination_chains）
from langchain.chains.router import MultiPromptChain

# 生成されたChainに入力を渡すことで、最初の入力を「default_chain」で受け取り、
# 「router_chain」でどのChainを使うか判断し、「destination_chains」の候補となるChainの一覧から指定のChainを実行し、
# 回答を生成する処理が内部的に行われる
chain = MultiPromptChain(
    router_chain=router_chain,
    destination_chains=destination_chains,
    default_chain=default_chain,
    verbose=True
)

# -----------------------------------------------------
# 7. MultiPromptChainに入力を渡し、LLMに使うChainを判断させて実行し、回答を得る
# -----------------------------------------------------
print(chain.run("社員の評価についてアドバイスをください。"))

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

destination_chains = {}
for prompt_info in prompt_infos:
    name = prompt_info["name"]
    prompt_template = prompt_info["prompt_template"]
    prompt = PromptTemplate(template=prompt_template)
    chain = LLMChain(llm=llm, prompt=prompt)
    destination_chains[name] = chain

destination_chains

default_prompt = PromptTemplate(input_variables=["input"], template="{input}")
default_chain = LLMChain(llm=llm, prompt=default_prompt)

destinations = []
for p in prompt_infos:
    destinations.append(f"{p['name']}: {p['description']}")

destinations

destinations_str = "\n".join(destinations)

print(destinations_str)

from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

print(MULTI_PROMPT_ROUTER_TEMPLATE)

router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations_str
)

from langchain.chains.router.llm_router import RouterOutputParser

router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)

from langchain.chains.router.llm_router import LLMRouterChain

router_chain = LLMRouterChain.from_llm(llm, router_prompt)

from langchain.chains.router import MultiPromptChain

chain = MultiPromptChain(
	 router_chain=router_chain,
	 destination_chains=destination_chains,
	 default_chain=default_chain,
	 verbose=True
)

chain.run("少し従業員同士の距離感が離れていっているのが課題です。アドバイスをください。")

chain.run("経費精算の方法を忘れたので教えて！")

"""#### 【解説】

「Chapter4: その他のChains」で解説した「LLMRouterChain」のコードと、処理の流れは全く同じです。各テーマごとのプロンプトのテンプレートと、それぞれの説明を辞書型で用意している箇所のデータが異なるだけです。

コードの意味を完璧に理解できずとも、「ユーザーの入力内容に応じて、使用するプロンプトのテンプレート（使用するChains）を分岐させる」という実装をしたい場面において、教材のコードをコピペしたものを、部分的に改変できるように処理の流れでもなんとなくは理解しておきましょう。
"""