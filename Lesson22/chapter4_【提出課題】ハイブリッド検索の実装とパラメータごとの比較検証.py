# -*- coding: utf-8 -*-
"""Chapter4:【提出課題】ハイブリッド検索の実装とパラメータごとの比較検証

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MKXk5vq0v2sDWHxDNcKSjk9IMU95sqXa

# Lesson22: 生成AIの評価と改善
# Chapter4:【提出課題】ハイブリッド検索の実装とパラメータごとの比較検証

事前準備を行った上で、提出課題に取り組みましょう。

**【提出方法】**  
Slackでメンターをメンションの上、当シート右上の「共有」からリンクをコピーし、提出してください。

## 事前準備

### 【OpenAI APIキーの設定】

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""ライブラリのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2

"""### 【データの用意】

サイドバーのファイルエリア上で「data」フォルダを作り、フォルダ内に以下3つのファイルをアップロードしましょう。  
※まずはGoogleドライブからダウンロードする必要があります。

**【ファイルが保存されているフォルダ】**  

「Lesson13: LangChainの主要モジュール4【Retrieval（RAG）】」フォルダ  
└「演習問題/提出課題」フォルダ  
└└「演習課題用データ」フォルダ

**【ファイル】**  
*   healthX_instructions.pdf（サービス詳細）
*   healthX_制作及び保守業務仕様書.pdf
*   20241207_MTG議事録_healthXのマーケティング施策について.pdf

## 提出課題

### 【問題文】

「Lesson13: Retrieval（RAG）」の演習問題5で扱ったRAGシステムについて、以下の条件でコード修正を行ってください。

**【条件】**
*   chain_typeの値の4種類と、モデルの2種類について、全ての組み合わせの計8パターンで  
回答時間と消費トークン数、利用料金を比較検証できるようにコードを修正してください。
*   モデルは「回答生成用に使うモデル」であり、「gpt-4o」と「gpt-4o-mini」の2種類を使ってください。
*   ベクトル検索ではなく、ハイブリッド検索の手法を使うようにコードを修正してください。

### 【修正用のコード】
"""

!pip install pymupdf==1.24.11 chromadb==0.5.11 tiktoken==0.8.0

# Commented out IPython magic to ensure Python compatibility.
# %pip install sudachipy sudachidict-full

# Commented out IPython magic to ensure Python compatibility.
# %pip install rank_bm25

from langchain_community.document_loaders import PyMuPDFLoader
import os
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback
import time
from sudachipy import tokenizer, dictionary
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

def preprocess_func(text):
    tokenizer_obj = dictionary.Dictionary(dict="full").create()
    mode = tokenizer.Tokenizer.SplitMode.A
    tokens = tokenizer_obj.tokenize(text ,mode)
    words = [token.surface() for token in tokens]
    words = list(set(words))
    return words

def generate_llm_response(chain_type, llm, retriever, query):
    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=chain_type, retriever=retriever)

    with get_openai_callback() as callback:
        result = chain.run(query)

        print(f"出力トークン数: {callback.completion_tokens}")
        print(f"入出力の合計トークン数: {callback.total_tokens}")
        print(f"料金（米ドル）: ${callback.total_cost:.6f}\n")

    return result

folder_name = "/content/data"
files = os.listdir(folder_name)

docs = []
for file in files:
    if not file.endswith(".pdf"):
        continue
    loader = PyMuPDFLoader(f"{folder_name}/{file}")
    pages = loader.load()
    docs.extend(pages)
text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=30,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(docs)
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(splitted_pages, embedding=embeddings)
retriever = db.as_retriever()

docs = []
for page in splitted_pages:
    docs.append(page.page_content)

bm25_retriever = BM25Retriever.from_texts(
    docs,
    preprocess_func=preprocess_func,
    k=3
)

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, retriever],
    weights=[0.5, 0.5]
)

chain_type_list = ["stuff", "map_reduce", "map_rerank", "refine"]

llm_gpt_4o = ChatOpenAI(model_name="gpt-4o", temperature=0.5)
llm_gpt_4o_mini = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
llm_dict = {"gpt-4o": llm_gpt_4o, "gpt-4o-mini": llm_gpt_4o_mini}

query = "HealthXの無料プランについて教えてください。"

for llm_name, llm in llm_dict.items():
    for chain_type in chain_type_list:
        print(f"モデル名: {llm_name}")
        print(f"chain_type: {chain_type}\n")

        start_time = time.time()
        result = generate_llm_response(chain_type, llm, ensemble_retriever, query)
        end_time = time.time()
        execution_time = end_time - start_time

        #print(f"●LLMからの回答\n{result}\n")
        print(f"実行時間: {execution_time}")
        print("====================\n\n")