# -*- coding: utf-8 -*-
"""【教材ソースコード】Lesson22: 生成AIの評価と改善

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_dXrYGg2xWPDj8UHevUA87lgvY6wvec2

# 【教材ソースコード】Lesson22: 生成AIの評価と改善

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""ライブラリのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2

"""## 教材サンプルコード

### Chapter4: 生成AIシステムの具体的な改善手法

---

#### パターンごとの検証

事前準備として、RAGシステム用のライブラリをインストール
"""

!pip install pymupdf==1.24.11 tiktoken==0.8.0 chromadb==0.5.11

"""##### chunk_sizeの探索

ライブラリ一式の読み込み
"""

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

"""どのchunk_sizeの値でも共通して使うオブジェクトの用意"""

loader = PyMuPDFLoader("healthX_instructions.pdf")
pages = loader.load()

embeddings = OpenAIEmbeddings()
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

"""各chunk_sizeごとに回答を生成する用の関数を定義"""

def generate_llm_response(chunk_size, pages, embeddings, llm, query):
    text_splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_size * 0.1,
        separator="\n"
    )
    splitted_pages = text_splitter.split_documents(pages)

    db = Chroma.from_documents(splitted_pages, embedding=embeddings)
    retriever = db.as_retriever()

    chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    result = chain.run(query)
    return result

"""用意したchunk_sizeごとに回答を生成するための各種データ準備"""

query = "HealthXの、ユーザー体験向上のための仕掛けを一言で教えてください。"
chunk_size_start = 100
chunk_size_end = 1000
chunk_size_interval = 100

"""各chunk_sizeごとに回答を生成"""

for chunk_size in range(chunk_size_start, chunk_size_end+1, chunk_size_interval):
    result = generate_llm_response(chunk_size, pages, embeddings, llm, query)

    print(f"●LLMからの回答（chunk_size: {chunk_size}）\n{result}\n")

"""##### 回答精度以外の検証

必要なライブラリの読み込み
"""

from langchain.callbacks import get_openai_callback
import time

"""どのパターンでも共通して使うオブジェクトの用意"""

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separator="\n"
)
splitted_pages = text_splitter.split_documents(pages)

db = Chroma.from_documents(splitted_pages, embedding=embeddings)
retriever = db.as_retriever()

"""モデルとchain_typeの各値の組み合わせごとに回答を生成する用の関数を定義"""

def generate_llm_response(chain_type, llm, retriever, query):
    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=chain_type, retriever=retriever)

    with get_openai_callback() as callback:
        result = chain.run(query)

        print(f"出力トークン数: {callback.completion_tokens}")
        print(f"入出力の合計トークン数: {callback.total_tokens}")
        print(f"料金（米ドル）: ${callback.total_cost:.6f}\n")

    return result

"""各パターンごとの比較検証をするための各種データ準備"""

query = "HealthXの、ユーザー体験向上のための仕掛けを一言で教えてください。"

chain_type_list = ["stuff", "map_reduce", "map_rerank", "refine"]

llm_gpt_4o_mini = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
llm_gpt_35_turbo = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
llm_dict = {
    "gpt-4o-mini": llm_gpt_4o_mini,
    "gpt-3.5-turbo": llm_gpt_35_turbo
}

"""用意したモデルとchain_typeの組み合わせごとに回答時間と料金を算出し、回答を生成"""

for llm_name, llm in llm_dict.items():
    for chain_type in chain_type_list:
        print(f"モデル名: {llm_name}")
        print(f"chain_type: {chain_type}\n")

        start_time = time.time()
        result = generate_llm_response(chain_type, llm, retriever, query)
        end_time = time.time()
        execution_time = end_time - start_time

        print(f"●LLMからの回答\n{result}\n")
        print(f"実行時間: {execution_time:.1f}")
        print("====================\n\n")

"""#### 複数の検索方法を組み合わせる

##### キーワード検索

必要なライブラリのインストール
"""

!pip install rank_bm25==0.2.2 sudachipy==0.6.10 sudachidict_full==20241021

"""データソースの読み込みとチャンク分割"""

loader = PyMuPDFLoader("healthX_instructions.pdf")
pages = loader.load()

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separator="\n",
)
splitted_pages = text_splitter.split_documents(pages)

"""検索対象のドキュメントを単語ごとに分割するための関数を定義"""

from typing import List
from sudachipy import tokenizer, dictionary

def preprocess_func(text):
    tokenizer_obj = dictionary.Dictionary(dict="full").create()
    mode = tokenizer.Tokenizer.SplitMode.A
    tokens = tokenizer_obj.tokenize(text ,mode)
    words = [token.surface() for token in tokens]
    words = list(set(words))
    return words

"""チャンク分割後の検索対象のドキュメントの  
各要素（Documentオブジェクト）からテキストを取り出し、順次リストに格納
"""

docs = []
for page in splitted_pages:
    docs.append(page.page_content)

"""BM25のRetrieverを作成"""

from langchain_community.retrievers import BM25Retriever

bm25_retriever = BM25Retriever.from_texts(
    docs,
    preprocess_func=preprocess_func,
    k=3
)

"""キーワード検索による関連性の高いドキュメントの取得"""

query = "HealthXの法人向けプランについて教えて"
bm25_retriever.invoke(query)

"""キーワード検索を活用した回答生成"""

chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=bm25_retriever)
result = chain.run(query)
print(result)

"""##### ハイブリッド検索

ハイブリッド検索用のRetriever（ツール）を作成
"""

from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, retriever],
    weights=[0.5, 0.5]
)

"""ハイブリッド検索による関連性の高いドキュメントの取得"""

query = "HealthXのセキュリティに関する情報を教えてください。"
ensemble_retriever.invoke(query)

"""ハイブリッド検索を活用した回答生成"""

chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=ensemble_retriever)
result = chain.run(query)
print(result)

"""### Chapter5: LLM as a Judgeによる評価

---

#### スコアによる評価

LLMに点数評価をしてもらうためのプロンプト
"""

prompt_template = """
以下に記載の「質問」に対しての「回答」について、
「ドキュメント」情報を参照した上で「評価観点」をもとに
理由と併せて「0〜100」で点数を付けてください。
誤りや不正確な情報が含まれている場合は、具体的に指摘してください。

【質問】
{question}

【回答】
{answer}

【評価観点】
1. 回答が提供されたドキュメントの情報に基づいているか？
2. ドキュメントと矛盾する内容が回答に含まれていないか？
3. 回答がドキュメントの文脈を正確に反映しているか？
4. 回答が質問に対して十分に網羅的であるか？

【ドキュメント】
{context}
"""

"""プロンプトのオブジェクトを作成"""

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["question", "context"],
    partial_variables={"answer": result}
)

"""後ほどRetrievalQAに渡すプロンプトのオブジェクトを設定"""

chain_type_kwargs = {"prompt": prompt}

"""評価用のChainの作成・実行"""

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs=chain_type_kwargs
)

query = "HealthXの、ユーザー体験向上のための仕掛けを教えてください。"
evaluation_result = chain.run(query)
print(evaluation_result)

"""#### 回答比較による評価

LLMに回答比較による評価をしてもらうためのプロンプト
"""

prompt_template = """
以下に質問と2つの回答があります。
それぞれの回答を以下の評価観点を元に評価し、どちらがより優れているかを選択してください。
また、より優れた回答であると判断した理由を説明してください。
さらに、選択された回答内容に誤りや不正確な情報が含まれている場合は、具体的に指摘してください。

【質問】
{question}

【回答1】
{answer1}

【回答2】
{answer2}

【評価観点】
1. 回答が提供されたドキュメントの情報に基づいているか？
2. ドキュメントと矛盾する内容が回答に含まれていないか？
3. 回答がドキュメントの文脈を正確に反映しているか？
4. 回答が質問に対して十分に網羅的であるか？

【ドキュメント】
{context}
"""

"""比較評価させる回答の用意"""

answer1 = """
「HealthX」では、ユーザーが健康維持に対して持続的な興味を持ち、日々の健康活動を積極的に行えるような仕掛けが用意されていますが、具体的な内容についてはわかりません。
"""

answer2 = """
HealthXでは、ユーザー体験向上のために以下のような仕掛けがあります。

1. **カスタマイズされたアドバイス**: AIを活用して、ユーザーごとに個別のフィードバックをリアルタイムで提供します。これにより、ユーザーは自分の健康状態を把握し、改善策を講じることができます。

2. **シンプルで直感的なユーザーインターフェース**: 操作が簡単で、誰でもすぐに使いこなせるように設計されています。

3. **ゲーミフィケーションの要素**: ユーザーが健康目標を達成するたびにポイントを獲得し、そのポイントをバッジや特典と交換できる仕組みがあり、健康管理を楽しめるようにしています。

4. **多様な問い合わせオプション**: 健康管理に関する疑問や技術的な問題について、さまざまなチャネルを通じてサポートを受けられる体制が整っています。

これらの仕掛けにより、ユーザーが健康維持に対して持続的な興味を持ち、日々の健康活動を楽しむことができるよう支援しています。
"""

"""プロンプトのオブジェクトを作成"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["question", "context"],
    partial_variables={"answer1": answer1, "answer2": answer2}
)

chain_type_kwargs = {"prompt": prompt}

"""評価用のChainの作成・実行"""

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs=chain_type_kwargs
)

query = "HealthXの、ユーザー体験向上のための仕掛けを教えてください。"
evaluation_result = chain.run(query)
print(evaluation_result)