# -*- coding: utf-8 -*-
"""【教材ソースコード】Lesson12: Memory

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ThkuHPO5BBGwb3UJMD5dnQo4vbfmkTfL

# 【教材ソースコード】Lesson12: LangChainの主要モジュール3【Memory】

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""各種パッケージのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 langchain-openai==0.2.2 httpx==0.27.2 pydantic==2.9.2

"""## 教材ソースコード

### Chapter2: Memoryの概要

---

会話履歴を踏まえた回答を自動的には行ってくれない例
"""

from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

messages1 = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="私の名前は田中です。"),
]
messages2 = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="私の名前を覚えていますか？"),
]

result1 = llm(messages1)
print(f"1回目: {result1.content}")

result2 = llm(messages2)
print(f"2回目: {result2.content}")

"""#### Chat Message History"""

from langchain.memory import ChatMessageHistory
from langchain.schema import SystemMessage, HumanMessage, AIMessage

memory = ChatMessageHistory()

memory.add_message(SystemMessage(content="You are a helpful assistant."))
memory.add_message(HumanMessage(content="私の名前は田中です。"))
memory.add_message(AIMessage(content="はい、田中さんですね。どのようにお手伝いしましょうか？"))
memory.add_message(HumanMessage(content="私の名前を覚えていますか？"))

messages = memory.messages
messages

"""回答生成"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
result = llm(messages)
result.content

"""#### Simple Memory"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from langchain.chains import SequentialChain
from langchain.memory import SimpleMemory

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

first_template = """
あなたは小説家です。以下のテーマで、{pre_audience}向けの小説の導入を300文字以内で書いてください。

テーマ：{theme}
トーン：{tone}
小説の導入：
"""

first_prompt = PromptTemplate(input_variables=["theme", "tone"], template=first_template,)
first_chain = LLMChain(llm=llm, prompt=first_prompt, output_key="pre_introduction")

second_template = """
あなたは小説家です。以下の小説の導入を{post_audience}向けに書き換えてください。

{pre_audience}向けに書かれた小説の導入：{pre_introduction}
{post_audience}向けの導入：
"""

second_prompt = PromptTemplate(input_variables=["pre_introduction"], template=second_template,)
second_chain = LLMChain(llm=llm, prompt=second_prompt, output_key="post_introduction")

chain = SequentialChain(
    chains=[first_chain, second_chain],
    input_variables=["theme", "tone"],
    output_variables=["pre_introduction", "post_introduction"],
    memory=SimpleMemory(memories={"pre_audience": "8歳の子供", "post_audience": "大人"}),
)

result = chain({"theme":"アンドロイドと人間の禁断の恋", "tone": "コメディ"})
result

result["pre_introduction"]

result["post_introduction"]

"""### Chapter3: Memoryの基本的な使い方

#### ConversationBufferMemory

コードの全体
"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory()

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

chain.predict(input="LangChainのMemoryについて100文字程度で説明してください。")

chain.predict(input="50文字以内で要約してください。")

"""コードの解説

各種ライブラリの読み込みと、モデル・Memoryのインスタンス作成
"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory()

"""会話履歴を取り出すコード"""

memory.load_memory_variables({})["history"]

"""ConversationChainのインスタンスを作成"""

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

"""1回目のLLMリクエスト"""

chain.predict(input="LangChainのMemoryについて100文字程度で説明してください。")

"""2回目のLLMリクエスト"""

chain.predict(input="50文字以内で要約してください。")

"""LLMリクエスト後の会話履歴の確認"""

memory.load_memory_variables({})["history"]

"""### Chapter4: Memoryの様々な種類

---

#### ConversationBufferWindowMemory
"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferWindowMemory(k=2)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="私の名前は田中です。")

"""2回目のLLMリクエスト"""

chain.predict(input="LangChainについて50文字以内で説明してください。")

"""3回目のLLMリクエスト"""

chain.predict(input="LangChainのMemoryについて50文字以内で説明してください。")

"""4回目のLLMリクエスト"""

chain.predict(input="ちなみに、私の名前を覚えていますか？")

"""#### ConversationTokenBufferMemory"""

!pip install tiktoken==0.8.0

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationTokenBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

"""トークン数を確認するための関数作成"""

import tiktoken

enc = tiktoken.get_encoding("cl100k_base")

tokens = enc.encode("私の名前は田中です。")

print(tokens)
print(len(tokens))

tokens_num = 0
def count_tokens(word, tokens_num):
    tokens = enc.encode(word)
    tokens_num += len(tokens)
    return tokens_num

"""1回目の入力"""

query = "私の名前は田中です。"

tokens_num = count_tokens(query, tokens_num)
print(f"消費トークン数: {tokens_num}")

result = chain.predict(input=query)
result

tokens_num = count_tokens(result, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""2回目の入力"""

query2 = "おすすめの本のジャンルを50文字以内で教えてください。"

tokens_num = count_tokens(query2, tokens_num)
print(f"消費トークン数: {tokens_num}")

result2 = chain.predict(input=query2)
print(result2)

tokens_num = count_tokens(result2, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""3回目の入力"""

chain.predict(input="私の名前を覚えていますか？")

"""#### ConversationSummaryBufferMemory"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

"""（1回目）LLMに渡すプロンプトのトークン数"""

query = "LangChainについて50文字以内で説明してください。"

tokens_num = 0
tokens_num = count_tokens(query, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""（1回目）LLMの回答結果を加算した合計消費トークン数"""

result = chain.predict(input=query)
print(result)

tokens_num = count_tokens(result, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""（2回目）LLMに渡すプロンプトを加算した合計消費トークン数"""

query2 = "LangChainのMemoryについて100文字以内で説明してください。"

tokens_num = count_tokens(query2, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""（2回目）LLMの回答結果を加算した合計消費トークン数"""

result2 = chain.predict(input=query2)
print(result2)

tokens_num = count_tokens(result2, tokens_num)
print(f"消費トークン数: {tokens_num}")

"""古い会話履歴が要約されることの確認"""

chain.predict(input="ここまでの会話履歴を踏まえ、LangChainと、LangChainのMemoryについて200文字程度で説明してください。")

"""古い会話履歴の要約についてさらに確認"""

chain.predict(input="ここまでの内容を50文字以内で日本語で要約してください。")

"""##### 日本語化

クラスの継承・オーバーライド
"""

from langchain_openai import ChatOpenAI
from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain

template = """
会話履歴と最新の入力をもとに、必要に応じて古い会話履歴を要約してください。

会話履歴
{summary}

最新の会話
{new_lines}

出力:
"""

prompt = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template=template
)

class CustomConversationSummaryBufferMemory(ConversationSummaryBufferMemory):
    prompt: PromptTemplate = prompt

"""日本語で回答が返ってくることの確認"""

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
memory = CustomConversationSummaryBufferMemory(llm=llm, max_token_limit=100)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="LangChainのMemoryについて200文字以内で説明してください。")

chain.predict(input="50文字程度で要約してください。")

"""#### VectorStoreRetrieverMemory

パッケージのインストール
"""

!pip install chromadb==0.5.11

"""必要なクラスを一式読み込む"""

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.memory import VectorStoreRetrieverMemory

"""テキストをベクトル化する準備"""

embeddings = OpenAIEmbeddings()

"""ベクターストアの用意"""

db = Chroma(embedding_function=embeddings)

"""Retrieverの作成"""

retriever = db.as_retriever(search_kwargs={"k":1})

"""Memoryインスタンスの作成"""

memory = VectorStoreRetrieverMemory(retriever=retriever)

"""1回目のLLMリクエスト"""

from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="英語の効率的な勉強方法を50文字以内で教えてください。")

"""2回目のLLMリクエスト"""

chain.predict(input="良い習慣を定着させるためのコツを50文字以内で教えてください。")

"""入力内容と関連性の高い会話履歴が取り出されることの確認（1回目）"""

chain.predict(input="英語の効率的な勉強方法について、もう一度教えてください。")

"""入力内容と関連性の高い会話履歴が取り出されることの確認（2回目）"""

chain.predict(input="健康的な体づくりのコツを50文字以内で教えてください。")

"""### Chapter5: MemoryとPromptsを組み合わせる

---

#### MemoryとPromptTemplate
"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain import PromptTemplate

template = """
あなたはLangChainの各モジュールに精通した専門アドバイザーです。
ユーザーの質問に応じて、Prompts、Chains、Memoryなどの各モジュールについて
詳細かつ実践的な回答を提供してください。

会話履歴:
{chat_history}

入力: {input}
出力:
"""

prompt = PromptTemplate(
    input_variables=["chat_history", "input"],
    template=template
)

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory(memory_key="chat_history")

chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True
)

chain.predict(input="LangChainのConversationBufferMemoryを使う際の注意点を、300文字以内で箇条書きで教えてください。")

chain.predict(input="注意点の中で特に重要なものを1つ挙げてください。")

"""#### MemoryとChatPromptTemplate

##### ChatPromptTemplateの振り返り
"""

from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

system_template = "あなたは、{genre}に詳しいAIです。ユーザーからの質問に100文字以内で回答してください。"
human_template = "{question}"

prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template(human_template),
])

messages = prompt.format_prompt(genre="フィットネス", question="健康的に体重を減らす方法を教えてください。").to_messages()
messages

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

result = llm(messages)
result.content

"""##### 基本的なコード"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage

system_template = """
あなたはPython初心者向けの演習問題を出題するAIアシスタントです。
データ型、条件分岐、繰り返し処理などの基本的なトピックをカバーする
シンプルな問題を作成してください。

【制約事項】
・与えられたテーマに対して、1つの演習問題のみ出題してください。
"""

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=system_template),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{input}"),
])

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory(return_messages=True)

chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True,
)

chain.predict(input="条件分岐")

"""2回目のLLM呼び出し"""

chain.predict(input="繰り返し処理")

"""##### 実用的なコード"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage

system_template = """
あなたはPython初心者向けの演習問題を出題するAIアシスタントです。
データ型、条件分岐、繰り返し処理などの基本的なトピックをカバーする
シンプルな問題を作成してください。

【制約事項】
・与えられたテーマに対して、1つの演習問題のみ出題してください。
・解答のヒントと解答例は、出題時には出さないでください。
・ユーザーから求められた場合に、解答のヒントと解答例を出す旨を出題時にのみ伝えてください。
・解答のヒントを求められた場合は、解答のヒントを出してください。
・解答例をを求められた場合は、解答例を出してください。
・Pythonのテーマが与えられた場合、毎回新しい問題を出題してください。
・ヒントや解答を求められた際は、新しい問題の出題はしないでください。
"""

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=system_template),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{input}"),
])

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory(return_messages=True)

chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory
)

count = 0
while True:
    count += 1
    if count == 1:
        input_message = "Pythonの学習テーマを入力してください（例: 条件分岐、繰り返し処理など）: "
    else:
        input_message = "ヒントは「1」、解答例は「2」、他の演習問題は「3」（「quit」で終了）: "
    input_data = input(input_message)

    if input_data.lower() == "quit":
        break

    if input_data == "1":
        input_message = "現在出題中の演習問題について、ヒントをください。"
    elif input_data == "2":
        input_message = "現在出題中の演習問題について、解答例をください。"
    elif input_data == "3":
        input_data = input("Pythonの学習テーマを入力してください（例: 条件分岐、繰り返し処理など）: ")

    print(chain.predict(input=input_data))