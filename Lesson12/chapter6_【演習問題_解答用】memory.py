# -*- coding: utf-8 -*-
"""Chapter6:【演習問題: 解答用】Memory

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hxXjj8GUAJwlF8SaJJOCl6YA0YObYCFt

# Lesson12: LangChainの主要モジュール3【Memory】
# Chapter6:【演習問題: 解答用】Memory

事前準備を行った上で、3つの演習問題に取り組みましょう。

各問題の「回答例/正解」と「解説」はデフォルトで非表示としていますが、  
非表示セルをクリックすれば確認できます。

まずは「回答例/正解」と「解説」を見ずにトライしてみましょう。

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""各種パッケージのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 langchain-openai==0.2.2 httpx==0.27.2 pydantic==2.9.2

"""## 演習問題

### 【問題1】
LLMに対して自己紹介し、その後何度かLLMとやり取りをした後、「私の名前を覚えていますか？」の問いかけに対して自分の名前を答えてもらえるよう、会話履歴を参照して回答するプログラムをLangChainのMemoryを使って作りましょう。

**【条件】**


*   使用するMemoryは「ConversationBufferMemory」とします。
*   使用するChainsは「ConversationChain」とします。
"""

# ConversationBufferMemoryとConversationChainを読み込む
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ChatOpenAIクラスのインスタンスを作成
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# ConversationBufferMemoryクラスのインスタンスを作成
# LLMとやり取りをするたび、このConversationBufferMemoryクラスのインスタンスに会話履歴が保存されていく
memory = ConversationBufferMemory()

# ConversationChainのインスタンスを作る。
# 引数にはChatOpenAIクラスのインスタンスと、Chainsの詳細ログを確認するための「verbose=True」に加え、
# memory引数にConversationBufferMemoryクラスのインスタンスを渡している。
# memory引数を指定することで、ChainsでLLMへの入出力が行われた際に、
# 入力と出力のそれぞれのデータがConversationChainのインスタンスに追加される。
chain = ConversationChain(
    llm=llm,
    memory=memory,
    # 「verbose=True」を指定することで、Chains内部の挙動を詳細に確認できる。
    verbose=True,
)

# ConversationChainのインスタンスに対して「predict()」メソッドを呼び出し、
# input引数にLLMへの入力を渡すことで回答を生成できる。
chain.predict(input="私は、大川雄一です。")

chain.predict(input="私は、野球に興味があります。最近は高校野球をよく見ます。")

chain.predict(input="私は、群馬出身なので、健大高崎を応援しています。")

chain.predict(input="私の名前を覚えていますか？")

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory()

chain = ConversationChain(
    llm=llm,
    memory=memory
)

print(chain.predict(input="私の名前は田中です。"))
print(chain.predict(input="好きな食べ物はカレーライスです。"))
print(chain.predict(input="寝る前にスマホを触るのをやめたいです。"))
print(chain.predict(input="でもなかなかやめられません。"))
print(chain.predict(input="私の名前を覚えていますか？"))

"""#### 【解説】

ConversationBufferMemoryは、会話履歴を全て記憶してプロンプトに含めるMemoryです。

ConversationChainと組み合わせて使うことで、簡単にLLMに会話履歴の記憶機能を追加です。

### 【問題2】
「問題1」のConversationBufferMemoryクラスのインスタンスには、会話履歴のデータが含まれています。

専用のメソッドを使い、会話履歴を全て表示してください。
"""

memory.load_memory_variables({})["history"]

"""#### 【解答例】"""

memory.load_memory_variables({})["history"]

"""#### 【解説】

Memoryのインスタンスに対して「load_memory_variables()」メソッドを呼び出し、  
引数に空の辞書を渡すことで、会話履歴のデータを辞書型で取り出せます。

会話履歴のテキストは「history」キーで取り出せます。

### 【問題3】
最新500トークンの会話履歴と、古い会話履歴の要約を参照して回答するプログラムを作り、  
何度かLLMとやり取りして古い会話履歴が要約されることを、  
Chain作成時に「verbose=True」を指定することで確認してください。

また回答が日本語で返される可能性が上がるよう、LLMへの会話履歴の要約リクエストを日本語化してください。
"""

!pip install tiktoken==0.8.0

from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory

# 会話履歴を「summary」、また最新の入力を「new_lines」という変数名としてPromptTemplateを作る。
template = """
会話履歴と最新の入力をもとに、必要に応じて古い会話履歴を要約してください。
会話履歴
{summary}
最新の入力
{new_lines}
出力:
"""

prompt = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template=template
)

# ConversationSummaryBufferMemoryクラスのpromptプロパティで英語の要約指示が設定されているため、
# ConversationSummaryBufferMemoryクラスの拡張クラスを定義し、promptプロパティを日本語での要約指示にオーバーライド（上書き）する。
class CustomConversationSummaryBufferMemory(ConversationSummaryBufferMemory):

    # promptプロパティで受け付ける引数のデータ型を
    # 「PromptTemplate」とし、用意したPromptTemplateで上書き
    prompt: PromptTemplate = prompt

# ------------------------------------------------------------------------------
# 3. Buffer Memory
#  3.4 ConversationSummaryBufferMemory
#   「最新Kトークン + 古い分の要約するMemory」
# ------------------------------------------------------------------------------
# クラス名が異なるだけで、それ以外はConversationTokenBufferMemoryの時と同じコード
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)


# ConversationSummaryBufferMemoryをインスタンス化する際、
# トークン数をカウントする対象のモデルを「llm」で指定し、「max_token_limit」では元の形のままで保持する最新トークン数を指定。
# トークン数が指定した500を超えたら、超えた範囲の分が要約され、システムメッセージとして会話履歴に追加される。
memory = CustomConversationSummaryBufferMemory(llm=llm, max_token_limit=500)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="ビッグバンについて500文字以内で説明してください。")

chain.predict(input="ここまでの内容を要約してください。")

chain.predict(input="ここまでの内容をさらに要約してください。")

memory.clear()

"""#### 【解答例】"""

!pip install tiktoken==0.8.0

from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory

template = """
会話履歴と最新の入力をもとに、必要に応じて古い会話履歴を要約してください。

会話履歴
{summary}

最新の会話
{new_lines}

出力:
"""

prompt = PromptTemplate(
    input_variables=["summary", "new_lines"],
    template=template
)

class CustomConversationSummaryBufferMemory(ConversationSummaryBufferMemory):
    prompt: PromptTemplate = prompt

from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=500)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

chain.predict(input="タスク管理の効果的な方法を教えてください。")

chain.predict(input="要約してください。")

chain.predict(input="さらに要約してください。")

"""#### 【解説】

指定トークン数を超える範囲の、古い会話履歴を要約してプロンプトに含める際に使うのは「ConversationSummaryBufferMemory」です。

日本語化対応は、ConversationSummaryBufferMemoryクラスのpromptプロパティをオーバーライドすることでできます。

### 【問題4】
会話履歴を記憶し、ユーザー入力内容と近い会話履歴を1つ取り出し、  
その会話履歴の情報をもとに回答する、LangChainのMemory機能を使ったプログラムを作ってください。

またChain作成時に「verbose=True」を指定し、ジャンルの異なる2つの質問を行った後、それら2つのうちどちらか一方に関連する質問を行うことで、入力内容と近い会話履歴が1つ取り出されていることを確認しましょう。
"""

!pip install chromadb==0.5.11

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.memory import VectorStoreRetrieverMemory

embeddings = OpenAIEmbeddings()
db = Chroma(embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k":1})
memory = VectorStoreRetrieverMemory(retriever=retriever)

# 用意したChainsの処理を走らせる。
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="日本で最も住みやすい地域を教えてください。")

chain.predict(input="世界の三大珍味を教えてください。")

chain.predict(input="日本で住みやすい地域をもう一度教えてください。")

chain.predict(input="フランス料理で人気のある料理を教えてください。")

chain.predict(input="九州で暮らしやすい地域を教えてください。")

"""#### 【解答例】"""

!pip install chromadb==0.5.11

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.memory import VectorStoreRetrieverMemory

embeddings = OpenAIEmbeddings()
db = Chroma(embedding_function=embeddings)
retriever = db.as_retriever(search_kwargs={"k":1})
memory = VectorStoreRetrieverMemory(retriever=retriever)

from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

chain.predict(input="仕事の効率を上げるシンプルな方法を50文字以内で教えてください。")

chain.predict(input="地球に優しい簡単なアクションを50文字以内で教えてください。")

chain.predict(input="忙しい日に優先順位をつけるコツを50文字以内で教えてください。")

chain.predict(input="プラスチック削減に役立つ日常の行動を50文字以内で教えてください。")

"""#### 【解説】"""

from langchain_openai import ChatOpenAI
from langchain.prompts.prompt import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain

template = """
会話履歴と最新の入力をもとに、必要に応じて古い会話履歴を要約してください。
会話履歴
{summary}
最新の入力
{new_lines}
出力:
"""

prompt = PromptTemplate(

    input_variables=["summary", "new_lines"],

    template=template

)

class CustomConversationSummaryBufferMemory(ConversationSummaryBufferMemory):

    prompt: PromptTemplate = prompt

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = CustomConversationSummaryBufferMemory(llm=llm, max_token_limit=100)

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True,
)

chain.predict(input="LangChainのMemoryについて200文字以内で説明してください。")
chain.predict(input="50文字程度で要約してください。")

"""入力内容と近い会話履歴を取り出す際に使うのは「VectorStoreRetrieverMemory」です。

会話履歴のテキストをベクトル化（数値のリストに変換）してデータベースに保管しておき、  
ユーザー入力のテキストをベクトル化した上で照合し、意味が近いものを取り出してプロンプトに含めることが可能です。

### 【問題5】
ConversationBufferMemoryとChatPromptTemplateを使い、LLMに会話履歴を記憶・参照して回答させるプログラムを作りましょう。

ただし入力はwhile文を使って何度も連続で行えるようにし、システムメッセージは以下とします。

**【システムメッセージ】**

あなたは親しみやすいAIアシスタントです。  
会話を通じてユーザーの趣味や好みを覚え、それを活かした質問や提案を行うことで、  
ユーザーとより良い関係を築いてください。
"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage

system_template = """
あなたは親しみやすいAIアシスタントです。
会話を通じてユーザーの趣味や好みを覚え、それを活かした質問や提案を行うことで、
ユーザーとより良い関係を築いてください。
"""

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=system_template),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{input}"),
])

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory(return_messages=True)

chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True,
)

count = 0

# while文で繰り返し処理を作り、ユーザー入力値として「quit」が入力されるまで何度も連続でチャットできる
while True:
    # ユーザー入力を受け付ける際、最初は「Pythonの学習テーマを入力してください」と表示
    input_message = "あなたの趣味や好みを入力してください（「quit」で終了）:"

    # 入力をinput関数で受け取る
    input_data = input(input_message)

    if input_data.lower() == "quit":
        break

    print(chain.predict(input=input_data))

"""#### 【解答例】"""

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage

system_template = """
あなたは親しみやすいAIアシスタントです。
会話を通じてユーザーの趣味や好みを覚え、それを活かした質問や提案を行うことで、
ユーザーとより良い関係を築いてください。
"""

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=system_template),
    MessagesPlaceholder(variable_name="history"),
    HumanMessagePromptTemplate.from_template("{input}"),
])

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory(return_messages=True)

chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory
)

while True:
    input_data = input("チャットAIと楽しく会話しましょう: ")
    if input_data.lower() == "quit":
        break
    print(chain.predict(input=input_data))

"""#### 【解説】

ChatPromptTemplateの「MessagesPlaceholder」を使うことで、会話履歴をプロンプトに含めることができます。

またMessagesPlaceholderを使う際、Memoryのクラスをインスタンス化する際に「return_messages=True」を指定する必要があります。
"""