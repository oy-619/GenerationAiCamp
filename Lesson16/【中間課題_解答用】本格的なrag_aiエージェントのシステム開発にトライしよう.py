# -*- coding: utf-8 -*-
"""【中間課題: 解答用】本格的なRAG/AIエージェントのシステム開発にトライしよう

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mRRh7nwBlt4uKeHkhBNbKvCmQqHjJsDC

# 【中間課題: 解答用】本格的なRAG/AIエージェントのシステム開発にトライしよう

## 事前準備
"""



"""### 【1. OpenAI APIキーの設定】

シークレット機能でOpenAI APIのAPIキーを設定しましょう。
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""### 【2. 必要なライブラリのインストール】

コース共通
"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2

"""中間課題用"""

!pip install python-docx==1.1.2 docx2txt==0.8 chromadb==0.5.23 tiktoken==0.8.0

"""### 【3. Googleドライブとの接続（マウント）】"""

from google.colab import drive
drive.mount('/content/drive')

"""### 【4. 外部参照先である「オンラインMTG議事録」フォルダのパス設定】"""

dir_path = "/content/drive/MyDrive/オンラインMTG議事録"

"""## 【中間課題①】オンラインMTG議事録を外部参照するRAGシステムの開発

### リセット処理（動作検証用）

「データベース化済み」フォルダと「.db」フォルダ内のファイルを全て削除する処理

**<font color= "red">※余裕がある方は、まずは自力での実装にトライしてみてください</font>**
"""

import shutil
import os

def clear_complete_dir(dir_path):
    print(dir_path)
    if os.path.isdir(dir_path):
        if dir_path.split("/")[-1] == ".db":
            shutil.rmtree(dir_path)
            os.mkdir(dir_path)
            print(f"作成済みの全てのベクターストアを削除しました。")
            return

    if os.path.isdir(dir_path):
        files = os.listdir(dir_path)
        if dir_path.split("/")[-1] == "データベース化済み":
            for file in files:
                os.remove(os.path.join(dir_path, file))
            print(f"対象テーマの「データベース化済み」フォルダを空にしました: {dir_path}")
            return
        for file in files:
            clear_complete_dir(os.path.join(dir_path, file))

# リセット処理の実行
clear_complete_dir(dir_path)

"""### ヒント

#### ヒント①（気軽に開いてOK）

RAGの外部参照先である「オンラインMTG議事録」フォルダ内の各テーマフォルダ内で、「データベース化前」フォルダ内にはあるが「データベース化済み」フォルダ内にはない議事録ファイルを読み込み、また「データベース化済み」フォルダ内にファイルをコピーする処理

**<font color= "red">※階層の深いフォルダ内のファイルを全て取得するためには、「再起処理」と呼ばれる少し複雑なコードを書く必要があります。  
実装イメージが湧く方、もしくは腕試しをしたい方は、自力での実装にトライしてみてください</font>**
"""

import os

def recursive_file_check(path, theme_docs):
    # 受け取ったパスがフォルダかどうかで条件分岐
    if os.path.isdir(path):
        # 「データベース化済み」もしくは「.db」フォルダの場合はファイル読み込みの処理が不要であるため、後続の処理をストップ
        if path.split("/")[-1] == "データベース化済み" or path.split("/")[-1] == ".db":
            return
        # フォルダ内のファイル/フォルダ名の一覧を取得
        files = os.listdir(path)
        # 一覧をループ処理
        for file in files:
            # フォルダ内のファイルもしくはフォルダのパスを渡し、再度関数を呼び出す（関数の中で同じ関数を呼び出す「再帰処理」）
            recursive_file_check(os.path.join(path, file), theme_docs)
    else:
        # 受け取ったパスがファイルの場合のみ、「読み込み」と「コピー」の処理を実行
        file_load(path, theme_docs)

from docx import Document
from langchain_community.document_loaders import Docx2txtLoader

def file_load(path, theme_docs):
    # パスを「/」で分割してリスト作成
    splitted_path = path.split("/")
    # パスからテーマ名を取り出し
    dir_path = "/content/drive/MyDrive/オンラインMTG議事録"
    theme_name = ""
    for path_elem in splitted_path:
        if path_elem in os.listdir(dir_path):
            theme_name = path_elem
            break
    # 「データベース化前」フォルダ内のファイルのパスであるため、「2つ階層を上げたところまでのパス」と「データベース化済み/ファイル名」のパスを連結
    save_filename = os.path.join("/".join(path.split("/")[:-2]), f"データベース化済み/{os.path.basename(path)}")
    # 「データベース化前」フォルダ内にはあるが「データベース化済み」フォルダ内にはない議事録ファイルの場合
    if not os.path.isfile(save_filename):
        # 拡張子「.docx」のファイルを読み込み
        loader = Docx2txtLoader(path)
        doc = loader.load()
        # 議事録ファイルを「データベース化済み」フォルダ内に保存
        new_doc = Document()
        new_doc.add_paragraph(doc[0].page_content)
        new_doc.save(save_filename)
        # テーマ名をキー、取得したドキュメントを値として辞書に追加
        if theme_name in theme_docs:
            theme_docs[theme_name] += doc
        else:
            theme_docs[theme_name] = doc

# 辞書「theme_docs」に、各テーマ名をキーとしてドキュメント一覧を格納する
theme_docs = {}
# 「オンラインMTG議事録」フォルダ内のファイル読み込みと「データベース化済み」フォルダへのコピーの処理を実行
recursive_file_check(dir_path, theme_docs)

# 辞書の中身を確認
theme_docs

"""#### ヒント②（基本開かない）

ベクターストア作成の事前準備

```Markdown
- CharacterTextSplitterクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Document transformers」を参照）
- OpenAIEmbeddingsクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Text embedding models」を参照）
- 7つのテーマ名一覧（「営業」など）が格納されたリストを作成
```

**<font color= "red">※上記ロジックを参照し、まずは自力での実装にトライしてみてください</font>**

##### 上記ロジックのコード
"""

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings

# CharacterTextSplitterクラスのインスタンスを作成
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
# OpenAIEmbeddingsクラスのインスタンスを作成
embeddings = OpenAIEmbeddings()

# 7つのテーマ名一覧（「営業」など）が格納されたリストを作成
theme_list = []
for filename in os.listdir(dir_path):
    if not filename.startswith('.'):
        theme_list.append(filename)

# リストの中身を確認
theme_list

"""#### ヒント③（最後の手段）

ベクターストアの作成と、すでに存在する場合は読み込んでデータを追加する処理

```Markdown
- 対象のベクターストアの種類
  - 全テーマ横断のベクターストア「.all_chromadb」
  - 各テーマごとのベクターストア「.テーマ名_chromadb」
```

**<font color= "red">※こちらのヒントには、中間課題を解くための最も重要なロジックとコードが書かれています。  
できる限り最後まで参照せず、トライしてみてください。</font>**
"""

from langchain.vectorstores import Chroma

# テーマ名をキー、Retrieverを値として追加する用の空の辞書を用意
theme_retriever = {}

"""テーマ名（「営業」など）の一覧が格納されたリストに対してfor文を回し、  
新たにベクターストアに追加するデータが該当テーマにおいて存在する場合のみ、for文の中で以下のロジックを実装する。

```Markdown
- 「オンラインMTG議事録」フォルダから読み込んだデータの中で、テーマ名に該当するデータを取り出し、チャンク分割を行う　（※Lesson13: Chapter3: 「Document transformers」を参照）
- テーマに対応するベクターストアが「.db」フォルダ内に存在するかどうかで条件分岐
  - 存在する場合
    - 該当テーマのベクターストアを読み込む　（※Lesson13: Chapter3: 「Vector Stores」を参照）
    - 該当テーマのベクターストアにチャンク分割したドキュメントを追加する　（※自力での調査が必要）
    - 全テーマ横断のベクターストアを読み込む　（※Lesson13: Chapter3: 「Vector Stores」を参照）
    - 全テーマ横断のベクターストアにチャンク分割したドキュメントを追加する　（※自力での調査が必要）
  - 存在しない場合
    - チャンク分割したドキュメントを使い、該当テーマのベクターストアを作成　（※Lesson13: Chapter3: 「Vector Stores」を参照）
    - 全テーマ横断のベクターストアが「.db」フォルダ内に存在するかどうかで条件分岐
      - 存在する場合
        - 全テーマ横断のベクターストアを読み込む　（※Lesson13: Chapter3: 「Vector Stores」を参照）
        - 全テーマ横断のベクターストアにチャンク分割したドキュメントを追加する　（※自力での調査が必要）
      - 存在しない場合
        - チャンク分割したドキュメントを使い、全テーマ横断のベクターストアを作成　（※Lesson13: Chapter3: 「Vector Stores」を参照）
- ベクターストアからRetrieverを作成し、テーマ名をキー、Retrieverを値として辞書に追加
```

最終的に作成される辞書「theme_retriever」のデータ構造

```Markdown
{
    '全社': 「.全社_chromadb」のRetriever,
    '教育': 「.教育_chromadb」のRetriever,
    'マーケティング': 「.マーケティング_chromadb」のRetriever,
    '営業': 「.営業_chromadb」のRetriever,
    '採用': 「.採用_chromadb」のRetriever,
    '開発': 「.開発_chromadb」のRetriever,
    '顧客': 「.顧客_chromadb」のRetriever
}
```

##### 上記ロジックのコード
**<font color= "red">※まずは上記のロジックをもとに、解答にトライしてみてください。  
ただし多くの試行錯誤を行ったがどうしても実装できず、想定学習時間を大幅に超過した場合のみ、  
最後の手段として上記ロジックの解答例の以下コードを参照してください。</font>**
"""

# テーマ名（「営業」など）の一覧が格納されたリストに対してfor文を回す
for theme_name in theme_list:
    # 新たにベクターストアに追加するデータが該当テーマにおいて存在する場合のみ、ベクターストアの作成やデータ追加の処理を実行
    if theme_name in theme_docs:
        # 「オンラインMTG議事録」フォルダから読み込んだデータの中で、テーマ名に該当するデータを取り出し、チャンク分割を行う
        splitted_docs = text_splitter.split_documents(theme_docs[theme_name])
        # テーマに対応するベクターストアが「.db」フォルダ内に存在するかどうかで条件分岐
        if os.path.isdir(f"{dir_path}/.db/.{theme_name}_chromadb"):
            # 該当テーマのベクターストアを読み込む
            db = Chroma(persist_directory=f"{dir_path}/.db/.{theme_name}_chromadb", embedding_function=embeddings)
            # 該当テーマのベクターストアにチャンク分割したドキュメントを追加する
            db.add_documents(documents=splitted_docs)
            # 全テーマ横断のベクターストアを読み込む
            all_db = Chroma(persist_directory=f"{dir_path}/.db/.all_chromadb", embedding_function=embeddings)
            # 全テーマ横断のベクターストアにチャンク分割したドキュメントを追加する
            all_db.add_documents(documents=splitted_docs)
        else:
            # チャンク分割したドキュメントを使い、該当テーマのベクターストアを作成
            db = Chroma.from_documents(splitted_docs, embeddings, persist_directory=f"{dir_path}/.db/.{theme_name}_chromadb")
            # 全テーマ横断のベクターストアが「.db」フォルダ内に存在するかどうかで条件分岐
            if os.path.isdir(f"{dir_path}/.db/.all_chromadb"):
                # 全テーマ横断のベクターストアを読み込む
                all_db = Chroma(persist_directory=f"{dir_path}/.db/.all_chromadb", embedding_function=embeddings)
                # 全テーマ横断のベクターストアにチャンク分割したドキュメントを追加する
                all_db.add_documents(documents=splitted_docs)
            else:
                # チャンク分割したドキュメントを使い、全テーマ横断のベクターストアを作成
                all_db = Chroma.from_documents(splitted_docs, embeddings, persist_directory=f"{dir_path}/.db/.all_chromadb")

        # ベクターストアからRetrieverを作成し、テーマ名をキー、Retrieverを値として辞書に追加
        theme_retriever[theme_name] = db.as_retriever()

# テーマ名をキー、Retrieverを値として追加した辞書の中身を確認
theme_retriever

"""#### ヒント④（ヒント①〜③の続きのコードがどうしても書けない場合に開く）

ヒント①〜③の続きで、LLMに回答を生成させるまでのロジック

**<font color= "red">※こちらのヒントには、ヒント①〜③の続きでLLMに回答を生成させるまでのロジックが書かれています。  
ここまで全てヒントを開くと「ほぼ中間課題の解答例」となります。  
そのためヒント①〜③を開いた場合、このヒントはできる限り最後まで開かずにトライしてみてください。</font>**

##### LLMに回答を生成させるまでのロジック

```Markdown
- 全テーマ横断のベクターストア「.all_chromadb」を読み込む　（※Lesson13: Chapter3: 「Vector Stores」を参照）
- Retrieverを作成する　（※Lesson13: Chapter3: 「Retrievers」を参照）
- ChatOpenAIクラスのインスタンスを、ストリーミング出力のオプション付きで作成する　（※Lesson15: Chapter2: 「各イベントのカスタム方法」を参照）
- 会話履歴の記憶機能が搭載されたRAGのChainを作成する　（※Lesson13: Chapter4を参照）
- 会話履歴を格納するための空のリストを用意する
- ユーザー入力値を変数に格納する
- ユーザー入力値と会話履歴をもとに回答を生成する　（※Lesson13: Chapter4を参照）
- 会話履歴のリストにユーザー入力値とLLMからの回答を追加する　（※Lesson13: Chapter4を参照）
- 再度ユーザー入力値をLLMに与え、会話履歴の記憶機能が搭載されていることを確認する
```

### 解答
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install "chromadb>=0.4.22,<0.5" "posthog<3.0"

# Commented out IPython magic to ensure Python compatibility.
# %pip install "numpy<2.0"

# --------------------------------------------------------------
# 1. インポート
# --------------------------------------------------------------
import os
import sys
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage
from langchain import LLMChain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_community.document_loaders import Docx2txtLoader
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.document_loaders import Docx2txtLoader
import shutil
from pathlib import Path
import unicodedata

def reset_files(root):
# 指定されたルートディレクトリ以下を再帰的に探索し、
# 削除対象ファイルを判定・削除する。
# 処理内容：
# - `iter_files()` を使って全ファイル・ディレクトリを走査
# - `is_target_file()` により削除対象を判定
# - 該当ファイルを `os.remove()` で削除
# - 削除・保持・失敗のログを標準出力／標準エラー出力に表示
# Args:
#     root (str): 探索を開始するルートディレクトリのパス
# Returns:
#     None
# Notes:
#     - 削除対象の判定ロジックは `is_target_file()` に委譲
#     - 削除失敗時は例外をキャッチしてエラーログを出力
#     - 実行前にバックアップを取ることを推奨
    print(f"\n=== リセット処理開始 ===\n対象ルート：{root}\n")
    for path, is_dir in iter_files(root):
        #if not is_dir and is_target_file(path):
        if is_dir and is_target_file(path):
            try:
                print(f"[削除実行] {path}")
                remove_all(path)
            except Exception as e:
                print(f"[削除失敗] {path}: {e}", file=sys.stderr)
        else:
            print(f"[保持] {path}")

    print("\n=== リセット処理完了 ===")

# 指定されたフォルダ配下のすべてのファイル・ディレクトリを削除する。
# フォルダ自体は残し、中身のみ削除します。
# Args:
#     target_dir (str): 削除対象のフォルダの絶対パス
# Returns:
#     None
# Notes:
#     - フォルダが存在しない場合は削除をスキップし、ログを表示します
#     - 削除失敗時は例外をキャッチしてエラーログを出力します
def remove_all(target_dir):
    """
    指定された「データベース化済み」フォルダ配下をすべて削除する。
    フォルダ自体は残し、中身のみ削除します。
    Args:
        target_dir (str): 削除対象の「データベース化済み」フォルダの絶対パス
    """
    if not os.path.exists(target_dir):
        print(f"[削除スキップ] フォルダが存在しません: {target_dir}")
        return
    for entry in os.listdir(target_dir):
        entry_path = os.path.join(target_dir, entry)
        try:
            if os.path.isdir(entry_path):
                shutil.rmtree(entry_path)
                print(f"[削除] ディレクトリ: {entry_path}")
            else:
                os.remove(entry_path)
                print(f"[削除] ファイル: {entry_path}")
        except Exception as e:
            print(f"[削除失敗] {entry_path}: {e}")

def iter_files(root, *, follow_symlinks=False):
# 指定されたルートディレクトリ以下を再帰的に探索し、
# 各ファイル・ディレクトリのパスと種別を yield する。
# Args:
#     root (str): 探索を開始するルートディレクトリのパス。
#     follow_symlinks (bool, optional): True の場合、
#     シンボリックリンクも辿って探索する。デフォルトは False。
# Yields:
#     tuple[str, bool]: 各エントリのパスと、ディレクトリかどうかを示すフラグ。
#                         例: ('/path/to/file.txt', False), ('/path/to/folder', True)
# Notes:
#     - アクセス権限がないディレクトリやファイルはスキップされ、標準エラー出力にログが表示されます。
#     - シンボリックリンクはデフォルトでは探索対象外です（follow_symlinks=False）。
#     - 再帰的にすべてのサブディレクトリを探索します。
    print(f"[探索開始] ルート：{root}")
    try:
        entries = os.listdir(root)
    except (PermissionError, FileNotFoundError) as e:
        print(f"[スキップ] {root} → アクセス不可: {e}", file=sys.stderr)
        return

    for name in entries:
        path = os.path.join(root, name)
        try:
            is_dir = os.path.isdir(path)
        except OSError as e:
            print(f"[スキップ] {path} → 判定失敗: {e}", file=sys.stderr)
            continue

        if is_dir:
            if not follow_symlinks and os.path.islink(path):
                print(f"[スキップ] シンボリックリンク（未追跡）：{path}")
                continue
            print(f"[ディレクトリ検出] {path}")
            yield (path, True)
            yield from iter_files(path, follow_symlinks=follow_symlinks)
        else:
            print(f"[ファイル検出] {path}")
            yield (path, False)

def is_target_file(path):
# 指定されたファイルパスが削除対象かどうかを判定する。
# 判定条件：
# - パスに「データベース化済み」または「.db」が含まれている
# - 拡張子が .docx または .db のいずれかである
# Args:
#     path (str): 判定対象のファイルパス
# Returns:
#     bool: 削除対象であれば True、対象外であれば False
# Notes:
#     - 判定結果はログとして標準出力に表示されます。
#     - Unicodeの「データベース化済み」表記に注意（全角文字や濁点の分離など）
    path = unicodedata.normalize("NFC", path)
    p = Path(path)
    if p.name == "データベース化済み" or p.name == ".db":
        print(f"[判定] 削除対象（フォルダ名に一致）：{path}")
        return True
    print(f"[判定] 削除対象外：{path}")
    return False

# 実行例（dir_path を明示的に定義）
    reset_files(dir_path)

# ファイル操作用
all_pages = []

# --------------------------------------------------------------
# 2. 変数定義
# --------------------------------------------------------------
# - CharacterTextSplitterクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Document transformers」を参照）
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30, separator="\n",)

# - OpenAIEmbeddingsクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Text embedding models」を参照）
embeddings = OpenAIEmbeddings()

# - 7つのテーマ名一覧（「営業」など）が格納されたリストを作成 = ["りんご", "バナナ", "みかん"]
theme_list = ["マーケティング", "全社", "営業", "採用", "教育", "開発", "顧客"]

# --------------------------------------------------------------
# 3. 関数定義
# --------------------------------------------------------------
def get_unprocessed_files(theme_root):
# テーマフォルダ配下を再帰的に探索し、「データベース化前」フォルダ内の未処理ファイルを抽出する。
# 処理内容：
# - 指定されたテーマフォルダ以下を再帰的に探索
# - 「データベース化前」フォルダを検出した場合、その中のすべてのファイルを対象に処理
# - 対応する「データベース化済み」フォルダの構造を再現し、同じ相対パスのファイルが存在しない場合に未処理と判定
# - 未処理ファイルのコピー元とコピー先のパスをペアとして返却
# Args:
#     theme_root (str): テーマフォルダのルートパス。例："/content/drive/MyDrive/オンラインMTG議事録/営業戦略"
# Returns:
#     List[Tuple[str, str]]: 未処理ファイルのペアリスト。各要素は (コピー元パス, コピー先パス)
# Notes:
#     - 「データベース化前」「データベース化済み」フォルダは同一階層に存在する前提で構築されます
#     - Unicode表記の「データベース化前」に注意（濁点分離など）
#     - コピー先フォルダが存在しない場合はこの関数では作成しません
    unprocessed = []

    # テーマフォルダ配下を再帰的に探索
    for dirpath, dirnames, filenames in os.walk(theme_root):
        # 「データベース化前」フォルダを見つけた場合のみ処理
        if os.path.basename(dirpath) == "データベース化前":
            # 対応する「データベース化済み」フォルダのパスを構築
            parent_dir = os.path.dirname(dirpath)
            dst_root = os.path.join(parent_dir, "データベース化済み")
            # 「データベース化前」フォルダ内のファイルを再帰的に探索
            for sub_dirpath, _, sub_filenames in os.walk(dirpath):
                for file in sub_filenames:
                    # コピー元ファイルの絶対パス
                    src_file = os.path.join(sub_dirpath, file)

                    # 「データベース化前」フォルダからの相対パスを取得
                    relative_path = os.path.relpath(src_file, dirpath)

                    # 同じ構造を「データベース化済み」側に再現
                    dst_file = os.path.join(dst_root, relative_path)

                    # 未処理ファイルなら追加
                    if not os.path.exists(dst_file):
                        unprocessed.append((src_file, dst_file))

    for src, dst in unprocessed:
        print(f"未処理ファイル：{src} → コピー先：{dst}")

    return unprocessed

def load_file_as_text(file_path):
# 指定された .docx ファイルを読み込み、LangChain の Document オブジェクトとして返す。
# 処理内容：
# - Docx2txtLoader を使用して Word 文書を読み込む
# - 文書内容を LangChain の Document オブジェクトとして構造化
# - 複数ページやセクションがある場合は、複数の Document に分割される可能性がある
# Args:
#     file_path (str): 読み込み対象の .docx ファイルの絶対パス
# Returns:
#     List[Document]: 読み込まれた文書の内容を含む Document オブジェクトのリスト
# Raises:
#     FileNotFoundError: 指定されたファイルが存在しない場合
#     ValueError: ファイル形式が .docx でない場合や読み込みに失敗した場合
# Notes:
#     - LangChain の Document オブジェクトは page_content と metadata を含む
#     - 読み込み後の内容はベクトル化や検索処理に利用可能
    loader = Docx2txtLoader(file_path)
    return loader.load()

def get_dbfile_name(path):
# 指定されたパスからテーマ名を抽出し、ChromaDB用の保存ファイル名を生成する。
# 処理内容：
# - `get_path_theme()` を使ってパスに含まれるテーマ名を取得
# - テーマ名が空でない場合、".{テーマ名}_chromadb" という形式のファイル名を返す
# Args:
#     path (str): テーマフォルダを含むファイルまたはディレクトリのパス
# Returns:
#     str or None: テーマ名に基づいたChromaDBファイル名（例：".営業戦略_chromadb"）。
#                     テーマが取得できない場合は None を返す（※現状は何も返さないため、明示的な else が必要）
# Notes:
#     - `get_path_theme()` がテーマ名を正しく抽出できる前提で動作します
#     - 返却されるファイル名は ChromaDB の `persist_directory` に使用される想定です
#     - 先頭のドット（"."）は隠しファイルとして扱われる可能性があるため、用途に応じて調整してください
# Example:
#     >>> get_dbfile_name("/content/drive/MyDrive/オンラインMTG議事録/営業戦略/データベース化前/mtg1.docx")
#     '.営業戦略_chromadb'
    theme = get_path_theme(path)
    if theme != "":
        return "." + theme + "_chromadb"

def get_path_theme(path):
# 指定されたパスに含まれるテーマ名を抽出する。
# 処理内容：
# - 事前に定義された `theme_list` に基づき、パスに含まれるテーマ名を検索
# - 最初に一致したテーマ名を返す。複数一致する場合は最初の1つのみ
# - 一致するテーマ名がない場合は空文字列を返す
# Args:
#     path (str): テーマフォルダを含むファイルまたはディレクトリのパス
# Returns:
#     str: 抽出されたテーマ名。見つからない場合は空文字列
# Notes:
#     - `theme_list` はグローバル変数として存在している前提
#     - 大文字・小文字は区別されます。必要に応じて正規化してください
# Example:
#     >>> get_path_theme("/content/drive/MyDrive/オンラインMTG議事録/営業戦略/mtg1.docx")
#     '営業戦略'
    for keyword in theme_list:
        if keyword in unicodedata.normalize("NFC", path):
            return keyword
    return ""

def is_path_theme(path):
# 指定されたパスにテーマ名が含まれているかどうかを判定する。
# 処理内容：
# - `get_path_theme()` を使ってパスに含まれるテーマ名を取得
# - テーマ名が空でない場合は True、空の場合は False を返す
# Args:
#     path (str): 判定対象のファイルまたはディレクトリのパス
# Returns:
#     bool: パスにテーマ名が含まれていれば True、含まれていなければ False
# Notes:
#     - `get_path_theme()` がテーマ名を正しく抽出できる前提で動作します
#     - テーマ名が複数含まれている場合でも、最初に見つかった1つのみで判定されます
    theme = get_path_theme(path)
    return theme != ""

def create_rag_chain(input_path):
# 指定されたテーマフォルダ内の未処理ファイルを読み込み、
# RAG（Retrieval-Augmented Generation）構造を持つChainを作成する。
# 処理内容：
# - `get_unprocessed_files()` により「データベース化前」フォルダ内の未処理ファイルを抽出
# - 各ファイルを Docx2txtLoader で読み込み、LangChain の Document オブジェクトとして取得
# - `text_splitter` により文書を分割し、ChromaDB に保存（テーマ別 or 全体DB）
# - LangChainの retriever を構築し、会話履歴対応の質問生成・応答Chainを構成
# - 最終的に、履歴対応のRAG Chain（retrieval_chain）を返す
# Args:
#     input_path (str): テーマフォルダのルートパス。例："/content/drive/MyDrive/オンラインMTG議事録/営業戦略"
# Returns:
#     RetrievalChain: LangChainの履歴対応RAG構造を持つChainオブジェクト
# Notes:
#     - `get_dbfile_name()` によりテーマ名に応じたDBファイル名を自動生成
#     - `all_pages` はグローバル変数として存在している前提（明示的な初期化が必要）
#     - `embeddings`, `text_splitter`, `theme_list` などの外部依存変数が事前に定義されている必要あり
#     - 会話履歴を活用するために `MessagesPlaceholder("chat_history")` を含むプロンプト構造を使用
#     - 処理対象が空の場合や読み込み失敗時は例外ログを出力するが、Chain自体は返される
# Raises:
#     Exception: ファイル読み込み・コピー・DB登録などの処理中に発生した例外は標準出力にログされるが、
#                関数内で明示的にraiseはされない
    pages = []

    # 指定テーマフォルダ内の未処理ファイルを抽出
    file_pairs = get_unprocessed_files(input_path)

    # 文書を分割
    text_splitter = CharacterTextSplitter(
        # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
        chunk_size=1000,
        # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
        chunk_overlap=50,
        # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
        separator="\n",
    )
    # OpenAIEmbeddingsクラスのインスタンスを作成
    embeddings = OpenAIEmbeddings()

    # 未処理ファイルのペア（コピー元とコピー先）を順に処理
    persist_directory = ""
    for src, dst in file_pairs:
        print(f"読み込み：{src} → 保存先：{dst}")
        try:
            # Docx2txtLoaderを使って.docxファイルを読み込む
            loader = Docx2txtLoader(src)
            # Documentオブジェクトのリストを返す
            page = loader.load()
            # 取得したDocumentをリストに追加
            pages.extend(page)
            all_pages.extend(page)
            # コピー先フォルダが存在しない場合は作成
            #os.makedirs(os.path.dirname(dst), exist_ok=True)
            # ファイルを「データベース化済み」フォルダにコピー
            shutil.copy2(src, dst)
            print(f"コピー完了：{dst}")
            dbfile_name = get_dbfile_name(src)
            # テキストを分割してChromaDBに保存
            splitted_pages = text_splitter.split_documents(pages)
            persist_directory=os.path.join(input_path + "/.db", dbfile_name)
            print(f"DB保存先：{persist_directory}")
            # すでにDBが存在する場合は追記、存在しない場合は新規作成
            db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=persist_directory)
            if os.path.isdir(persist_directory):
                # 新しい文書を足すときは add_documents を使う
                db.add_documents(splitted_pages)
                db.persist()
            # すでにDBが存在する場合は追記、存在しない場合は新規作成
            else:
                db.persist()
            # 全体用DBにも追加
            splitted_pages = text_splitter.split_documents(all_pages)
            persist_directory = os.path.join(dir_path + "/.db", ".all_chromadb")
            if os.path.isdir(persist_directory):
                db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=persist_directory)
                db.add_documents(splitted_pages)
                db.persist()
            else:
                db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=persist_directory)
                db.persist()
        except Exception as e:
            # 読み込みやコピーに失敗した場合はエラーログを表示
            print(f"[エラー] : {e}")

    # 既存のChromaDBインスタンスを取得
    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    retriever = db.as_retriever()
    question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    question_answer_template = """
    あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
    また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"
    {context}
    """

    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, question_generator_prompt
    )

    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)

    #question_answer_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return chain

# --------------------------------------------------------------
# 4. メイン処理
# --------------------------------------------------------------
def main():

    # グローバル変数の初期化
    chat_history = []
    # チェーンの作成
    chain = create_rag_chain(dir_path)
    # 質問内容
    query = "多くの企業が力を入れているマーケティング施策は何ですか？"
    # チェーンの実行
    ai_msg = chain.invoke({"input": query, "chat_history": chat_history})
    # 応答内容を表示
    print(ai_msg["answer"])
    # 会話履歴に追加
    chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

    # 追加の質問
    query = "100文字以内に要約して"
    # チェーンの実行
    ai_msg = chain.invoke({"input": query, "chat_history": chat_history})
    # 応答内容を表示
    print(ai_msg["answer"])

# --------------------------------------------------------------
# 5. エントリポイント
# --------------------------------------------------------------
if __name__ == "__main__":
    main()

"""## 【中間課題②】作成したRAGシステムにAIエージェント機能を搭載する

### 解答
"""

# --------------------------------------------------------------
# 1. インポート
# --------------------------------------------------------------
import os
import sys
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage
from langchain import LLMChain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import Docx2txtLoader
from langchain.document_loaders import Docx2txtLoader
import shutil
from langchain.agents import AgentType, initialize_agent, load_tools
from pathlib import Path
import unicodedata
from langchain.tools import Tool

# --------------------------------------------------------------
# 2. 変数定義
# --------------------------------------------------------------

# 会話履歴用の変数定義
# - 各テーマごとに履歴を保持するためのリストを用意
marketing_chain_chat__history = []      # 1.マーケティング
wholecompany_chain_chat_history = []    # 2.全社
sales_chain_chat_history = []           # 3.営業
recruitment_chain_chat_history = []     # 4.採用
education_chain_chat_history = []       # 5.教育
development_chain_chat_history = []     # 6.開発
client_chain_chat_history = []          # 7.顧客

# ファイル操作用
all_pages = []

# - 議事録ファイルが格納されているディレクトリのパスを変数に格納
#dir_path = r"C:\work\ws_python\GenerationAiCamp\オンラインMTG議事録"

# - CharacterTextSplitterクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Document transformers」を参照）
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30, separator="\n",)

# - OpenAIEmbeddingsクラスのインスタンスを作成　（※Lesson13: Chapter3: 「Text embedding models」を参照）
embeddings = OpenAIEmbeddings()

# - 7つのテーマ名一覧（「営業」など）が格納されたリストを作成 = ["りんご", "バナナ", "みかん"]
theme_list = ["マーケティング", "全社", "営業", "採用", "教育", "開発", "顧客"]

# --------------------------------------------------------------
# 3. 関数定義
# --------------------------------------------------------------
def iter_files(root, *, follow_symlinks=False):
# 指定されたルートディレクトリ以下を再帰的に探索し、
# 各ファイル・ディレクトリのパスと種別を yield する。
# Args:
#     root (str): 探索を開始するルートディレクトリのパス。
#     follow_symlinks (bool, optional): True の場合、
#     シンボリックリンクも辿って探索する。デフォルトは False。
# Yields:
#     tuple[str, bool]: 各エントリのパスと、ディレクトリかどうかを示すフラグ。
#                         例: ('/path/to/file.txt', False), ('/path/to/folder', True)
# Notes:
#     - アクセス権限がないディレクトリやファイルはスキップされ、標準エラー出力にログが表示されます。
#     - シンボリックリンクはデフォルトでは探索対象外です（follow_symlinks=False）。
#     - 再帰的にすべてのサブディレクトリを探索します。
    print(f"[探索開始] ルート：{root}")
    try:
        entries = os.listdir(root)
    except (PermissionError, FileNotFoundError) as e:
        print(f"[スキップ] {root} → アクセス不可: {e}", file=sys.stderr)
        return

    for name in entries:
        path = os.path.join(root, name)
        try:
            is_dir = os.path.isdir(path)
        except OSError as e:
            print(f"[スキップ] {path} → 判定失敗: {e}", file=sys.stderr)
            continue

        if is_dir:
            if not follow_symlinks and os.path.islink(path):
                print(f"[スキップ] シンボリックリンク（未追跡）：{path}")
                continue
            print(f"[ディレクトリ検出] {path}")
            yield (path, True)
            yield from iter_files(path, follow_symlinks=follow_symlinks)
        else:
            print(f"[ファイル検出] {path}")
            yield (path, False)

def is_target_file(path):
# 指定されたファイルパスが削除対象かどうかを判定する。
# 判定条件：
# - パスに「データベース化済み」または「.db」が含まれている
# - 拡張子が .docx または .db のいずれかである
# Args:
#     path (str): 判定対象のファイルパス
# Returns:
#     bool: 削除対象であれば True、対象外であれば False
# Notes:
#     - 判定結果はログとして標準出力に表示されます。
#     - Unicodeの「データベース化済み」表記に注意（全角文字や濁点の分離など）
    path = unicodedata.normalize("NFC", path)
    p = Path(path)
    if p.name == "データベース化済み" or p.name == ".db":
        print(f"[判定] 削除対象（フォルダ名に一致）：{path}")
        return True
    print(f"[判定] 削除対象外：{path}")
    return False

def reset_files(root):
# 指定されたルートディレクトリ以下を再帰的に探索し、
# 削除対象ファイルを判定・削除する。
# 処理内容：
# - `iter_files()` を使って全ファイル・ディレクトリを走査
# - `is_target_file()` により削除対象を判定
# - 該当ファイルを `os.remove()` で削除
# - 削除・保持・失敗のログを標準出力／標準エラー出力に表示
# Args:
#     root (str): 探索を開始するルートディレクトリのパス
# Returns:
#     None
# Notes:
#     - 削除対象の判定ロジックは `is_target_file()` に委譲
#     - 削除失敗時は例外をキャッチしてエラーログを出力
#     - 実行前にバックアップを取ることを推奨
    print(f"\n=== リセット処理開始 ===\n対象ルート：{root}\n")
    for path, is_dir in iter_files(root):
        #if not is_dir and is_target_file(path):
        if is_dir and is_target_file(path):
            try:
                print(f"[削除実行] {path}")
                remove_all(path)
            except Exception as e:
                print(f"[削除失敗] {path}: {e}", file=sys.stderr)
        else:
            print(f"[保持] {path}")

    print("\n=== リセット処理完了 ===")

# 指定されたフォルダ配下のすべてのファイル・ディレクトリを削除する。
# フォルダ自体は残し、中身のみ削除します。
# Args:
#     target_dir (str): 削除対象のフォルダの絶対パス
# Returns:
#     None
# Notes:
#     - フォルダが存在しない場合は削除をスキップし、ログを表示します
#     - 削除失敗時は例外をキャッチしてエラーログを出力します
def remove_all(target_dir):
    """
    指定された「データベース化済み」フォルダ配下をすべて削除する。
    フォルダ自体は残し、中身のみ削除します。
    Args:
        target_dir (str): 削除対象の「データベース化済み」フォルダの絶対パス
    """
    if not os.path.exists(target_dir):
        print(f"[削除スキップ] フォルダが存在しません: {target_dir}")
        return
    for entry in os.listdir(target_dir):
        entry_path = os.path.join(target_dir, entry)
        try:
            if os.path.isdir(entry_path):
                shutil.rmtree(entry_path)
                print(f"[削除] ディレクトリ: {entry_path}")
            else:
                os.remove(entry_path)
                print(f"[削除] ファイル: {entry_path}")
        except Exception as e:
            print(f"[削除失敗] {entry_path}: {e}")

def get_unprocessed_files(theme_root, theme):
# テーマフォルダ配下を再帰的に探索し、「データベース化前」フォルダ内の未処理ファイルを抽出する。
# 処理内容：
# - 指定されたテーマフォルダ以下を再帰的に探索
# - 「データベース化前」フォルダを検出した場合、その中のすべてのファイルを対象に処理
# - 対応する「データベース化済み」フォルダの構造を再現し、同じ相対パスのファイルが存在しない場合に未処理と判定
# - 未処理ファイルのコピー元とコピー先のパスをペアとして返却
# Args:
#     theme_root (str): テーマフォルダのルートパス。例："/content/drive/MyDrive/オンラインMTG議事録/営業戦略"
# Returns:
#     List[Tuple[str, str]]: 未処理ファイルのペアリスト。各要素は (コピー元パス, コピー先パス)
# Notes:
#     - 「データベース化前」「データベース化済み」フォルダは同一階層に存在する前提で構築されます
#     - Unicode表記の「データベース化前」に注意（濁点分離など）
#     - コピー先フォルダが存在しない場合はこの関数では作成しません
    unprocessed = []

    # テーマフォルダ配下を再帰的に探索
    for dirpath, dirnames, filenames in os.walk(theme_root):
        # 指定されたテーマフォルダ以外はスキップ
        normalized = unicodedata.normalize("NFC", dirpath)
        if theme not in normalized:
            continue

        # 「データベース化前」フォルダを見つけた場合のみ処理
        if os.path.basename(dirpath) == "データベース化前":
            # 対応する「データベース化済み」フォルダのパスを構築
            parent_dir = os.path.dirname(dirpath)
            dst_root = os.path.join(parent_dir, "データベース化済み")
            # 「データベース化前」フォルダ内のファイルを再帰的に探索
            for sub_dirpath, _, sub_filenames in os.walk(dirpath):
                for file in sub_filenames:
                    # コピー元ファイルの絶対パス
                    src_file = os.path.join(sub_dirpath, file)

                    # 「データベース化前」フォルダからの相対パスを取得
                    relative_path = os.path.relpath(src_file, dirpath)

                    # 同じ構造を「データベース化済み」側に再現
                    dst_file = os.path.join(dst_root, relative_path)

                    # 未処理ファイルなら追加
                    if not os.path.exists(dst_file):
                        unprocessed.append((src_file, dst_file))

    for src, dst in unprocessed:
        print(f"未処理ファイル：{src} → コピー先：{dst}")

    return unprocessed

def load_file_as_text(file_path):
# 指定された .docx ファイルを読み込み、LangChain の Document オブジェクトとして返す。
# 処理内容：
# - Docx2txtLoader を使用して Word 文書を読み込む
# - 文書内容を LangChain の Document オブジェクトとして構造化
# - 複数ページやセクションがある場合は、複数の Document に分割される可能性がある
# Args:
#     file_path (str): 読み込み対象の .docx ファイルの絶対パス
# Returns:
#     List[Document]: 読み込まれた文書の内容を含む Document オブジェクトのリスト
# Raises:
#     FileNotFoundError: 指定されたファイルが存在しない場合
#     ValueError: ファイル形式が .docx でない場合や読み込みに失敗した場合
# Notes:
#     - LangChain の Document オブジェクトは page_content と metadata を含む
#     - 読み込み後の内容はベクトル化や検索処理に利用可能
    loader = Docx2txtLoader(file_path)
    return loader.load()

def get_dbfile_name(path):
# 指定されたパスからテーマ名を抽出し、ChromaDB用の保存ファイル名を生成する。
# 処理内容：
# - `get_path_theme()` を使ってパスに含まれるテーマ名を取得
# - テーマ名が空でない場合、".{テーマ名}_chromadb" という形式のファイル名を返す
# Args:
#     path (str): テーマフォルダを含むファイルまたはディレクトリのパス
# Returns:
#     str or None: テーマ名に基づいたChromaDBファイル名（例：".営業戦略_chromadb"）。
#                     テーマが取得できない場合は None を返す（※現状は何も返さないため、明示的な else が必要）
# Notes:
#     - `get_path_theme()` がテーマ名を正しく抽出できる前提で動作します
#     - 返却されるファイル名は ChromaDB の `persist_directory` に使用される想定です
#     - 先頭のドット（"."）は隠しファイルとして扱われる可能性があるため、用途に応じて調整してください
# Example:
#     >>> get_dbfile_name("/content/drive/MyDrive/オンラインMTG議事録/営業戦略/データベース化前/mtg1.docx")
#     '.営業戦略_chromadb'
    theme = get_path_theme(path)
    if theme != "":
        return "." + theme + "_chromadb"

def get_path_theme(path):
# 指定されたパスに含まれるテーマ名を抽出する。
# 処理内容：
# - 事前に定義された `theme_list` に基づき、パスに含まれるテーマ名を検索
# - 最初に一致したテーマ名を返す。複数一致する場合は最初の1つのみ
# - 一致するテーマ名がない場合は空文字列を返す
# Args:
#     path (str): テーマフォルダを含むファイルまたはディレクトリのパス
# Returns:
#     str: 抽出されたテーマ名。見つからない場合は空文字列
# Notes:
#     - `theme_list` はグローバル変数として存在している前提
#     - 大文字・小文字は区別されます。必要に応じて正規化してください
# Example:
#     >>> get_path_theme("/content/drive/MyDrive/オンラインMTG議事録/営業戦略/mtg1.docx")
#     '営業戦略'
    for keyword in theme_list:
        if keyword in unicodedata.normalize("NFC", path):
            return keyword
    return ""

def is_path_theme(path):
# 指定されたパスにテーマ名が含まれているかどうかを判定する。
# 処理内容：
# - `get_path_theme()` を使ってパスに含まれるテーマ名を取得
# - テーマ名が空でない場合は True、空の場合は False を返す
# Args:
#     path (str): 判定対象のファイルまたはディレクトリのパス
# Returns:
#     bool: パスにテーマ名が含まれていれば True、含まれていなければ False
# Notes:
#     - `get_path_theme()` がテーマ名を正しく抽出できる前提で動作します
#     - テーマ名が複数含まれている場合でも、最初に見つかった1つのみで判定されます
    theme = get_path_theme(path)
    return theme != ""

def create_rag_chain(theme):
# 指定されたテーマフォルダ内の未処理ファイルを読み込み、
# RAG（Retrieval-Augmented Generation）構造を持つChainを作成する。
# 処理内容：
# - `get_unprocessed_files()` により「データベース化前」フォルダ内の未処理ファイルを抽出
# - 各ファイルを Docx2txtLoader で読み込み、LangChain の Document オブジェクトとして取得
# - `text_splitter` により文書を分割し、ChromaDB に保存（テーマ別 or 全体DB）
# - LangChainの retriever を構築し、会話履歴対応の質問生成・応答Chainを構成
# - 最終的に、履歴対応のRAG Chain（retrieval_chain）を返す
# Args:
#     theme (str): テーマ。例："営業戦略"
# Returns:
#     RetrievalChain: LangChainの履歴対応RAG構造を持つChainオブジェクト
# Notes:
#     - `get_dbfile_name()` によりテーマ名に応じたDBファイル名を自動生成
#     - `all_pages` はグローバル変数として存在している前提（明示的な初期化が必要）
#     - `embeddings`, `text_splitter`, `theme_list` などの外部依存変数が事前に定義されている必要あり
#     - 会話履歴を活用するために `MessagesPlaceholder("chat_history")` を含むプロンプト構造を使用
#     - 処理対象が空の場合や読み込み失敗時は例外ログを出力するが、Chain自体は返される
# Raises:
#     Exception: ファイル読み込み・コピー・DB登録などの処理中に発生した例外は標準出力にログされるが、
#                関数内で明示的にraiseはされない
    pages = []

    # テーマフォルダのパスを構築
    input_path = os.path.join(dir_path, theme)

    # 指定テーマフォルダ内の未処理ファイルを抽出
    file_pairs = get_unprocessed_files(dir_path, theme)

    # 文書を分割
    text_splitter = CharacterTextSplitter(
        # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
        chunk_size=1000,
        # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
        chunk_overlap=50,
        # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
        separator="\n",
    )

    # OpenAIEmbeddingsクラスのインスタンスを作成
    embeddings = OpenAIEmbeddings()

    # 未処理ファイルのペア（コピー元とコピー先）を順に処理
    persist_directory = ""
    for src, dst in file_pairs:
        print(f"読み込み：{src} → 保存先：{dst}")
        try:
            # Docx2txtLoaderを使って.docxファイルを読み込む
            loader = Docx2txtLoader(src)
            # Documentオブジェクトのリストを返す
            page = loader.load()
            # 取得したDocumentをリストに追加
            pages.extend(page)
            all_pages.extend(page)
            # ファイルを「データベース化済み」フォルダにコピー
            shutil.copy2(src, dst)
            print(f"コピー完了：{dst}")
            # テキストを分割してChromaDBに保存
            splitted_pages = text_splitter.split_documents(pages)
            dbfile_name = get_dbfile_name(src)
            persist_directory=os.path.join(dir_path + "/.db", dbfile_name)
            print(f"DB保存先：{persist_directory}")
            # すでにDBが存在する場合は追記、存在しない場合は新規作成
            if os.path.isdir(persist_directory):
                # 新しい文書を足すときは add_documents を使う
                db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
                db.add_documents(splitted_pages)
                db.persist()
            # すでにDBが存在する場合は追記、存在しない場合は新規作成
            else:
                db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=persist_directory)
                db.persist()
            # 全体用DBにも追加
            splitted_pages = text_splitter.split_documents(all_pages)
            all_persist_directory = os.path.join(dir_path + "/.db", ".all_chromadb")
            if os.path.isdir(all_persist_directory):
                all_db = Chroma(persist_directory=all_persist_directory, embedding_function=embeddings)
                all_db.add_documents(splitted_pages)
                all_db.persist()
            else:
                all_db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=all_persist_directory)
                all_db.persist()
        except Exception as e:
            # 読み込みやコピーに失敗した場合はエラーログを表示
            print(f"[エラー] : {e}")

    # 未処理ファイルがない場合でも既存のDBを使ってChainを作成
    if file_pairs == []:
        for t in theme_list:
            if t == theme:
                dbfile_name = "." + theme + "_chromadb"
                persist_directory=os.path.join(dir_path + "/.db", dbfile_name)
                print(f"DB保存先：{persist_directory}")
                break

    # ChromaDBのRetrieverを作成し、履歴対応のRAG Chainを構築
    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    # Retrieverを作成
    retriever = db.as_retriever()
    # 会話履歴対応のRAG Chainを作成するためのプロンプトテンプレートを定義
    question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"
    # MessagesPlaceholderを使って会話履歴を埋め込む
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    question_answer_template = """
    あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
    また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"
    {context}
    """

    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # ChatOpenAIクラスのインスタンスを作成（モデル名や温度パラメータを指定）
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

    # create_history_aware_retriever() 関数を使って履歴対応のRetrieverを生成
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, question_generator_prompt
    )

    # 質問応答用のChainを作成
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # 履歴対応のRAG Chainを作成
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return chain

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_marketing_theme_chain(param):
# 変数「marketing_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「marketing_theme_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「marketing_theme_chain」と「marketing_theme_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「marketing_theme_chain_chat_history」に追加。
    ai_msg = marketing_theme_chain.invoke({"input": param, "chat_history": marketing_chain_chat__history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    marketing_chain_chat__history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_wholecompany_theme_chain(param):
# 変数「wholecompany_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「wholecompany_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「wholecompany_theme_chain」と「wholecompany_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = wholecompany_theme_chain.invoke({"input": param, "chat_history": wholecompany_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    wholecompany_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_sales_theme_chain(param):
# 変数「sales_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「sales_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「sales_theme_chain」と「sales_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = sales_theme_chain.invoke({"input": param, "chat_history": sales_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    sales_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_recruitment_theme_chain(param):
# 変数「recruitment_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「recruitment_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「recruitment_theme_chain」と「recruitment_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = recruitment_theme_chain.invoke({"input": param, "chat_history": recruitment_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    recruitment_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]


# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_education_theme_chain(param):
# 変数「education_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「education_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「education_theme_chain」と「education_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = education_theme_chain.invoke({"input": param, "chat_history": education_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    education_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_development_theme_chain(param):
# 変数「development_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「development_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「development_theme_chain」と「development_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = development_theme_chain.invoke({"input": param, "chat_history": development_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    development_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化した
# Chains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_client_theme_chain(param):
# 変数「client_theme_chain」に格納されたChainを実行する関数
# - 引数「param」に質問内容を指定
# - LLMが生成した回答を返す
# - 会話履歴はグローバル変数「client_chain_chat_history」に保存
# Args:
#     param (str): LLMに対する質問内容
# Returns:
#     str: LLMが生成した回答
# Notes:
#     - 事前に「client_theme_chain」と「client_chain_chat_history」が定義されている必要があります
    # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
    ai_msg = client_theme_chain.invoke({"input": param, "chat_history": client_chain_chat_history})
    # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
    client_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
    return ai_msg["answer"]

# --------------------------------------------------------------
# 4. メイン処理
# --------------------------------------------------------------
def main():

    # marketingフォルダのRAG化したChainを格納する変数
    global marketing_theme_chain
    # 全社フォルダのRAG化したChainを格納する変数
    global wholecompany_theme_chain
    # 営業フォルダのRAG化したChainを格納する変数
    global sales_theme_chain
    # 採用フォルダのRAG化したChainを格納する変数
    global recruitment_theme_chain
    # 教育フォルダのRAG化したChainを格納する変数
    global education_theme_chain
    # 開発フォルダのRAG化したChainを格納する変数
    global development_theme_chain
    # 顧客フォルダのRAG化したChainを格納する変数
    global client_theme_chain

    # リセット処理
    #reset_files(dir_path)

    # ファイル別にChainsを作成
    # 1.マーケティング
    marketing_theme_chain = create_rag_chain("マーケティング")
    # 2.全社
    wholecompany_theme_chain = create_rag_chain("全社")
    # 3.営業
    sales_theme_chain = create_rag_chain("営業")
    # 4.採用
    recruitment_theme_chain = create_rag_chain("採用")
    # 5.教育
    education_theme_chain = create_rag_chain("教育")
    # 6.開発
    development_theme_chain = create_rag_chain("開発")
    # 7.顧客
    client_theme_chain = create_rag_chain("顧客")

    marketing_theme_tool = Tool.from_function(
        func=run_marketing_theme_chain,
        name="マーケティング議事録検索",
        description=(
            "質問内容がマーケティング、広告、プロモーション、販促、ブランド、集客、"
            "市場調査、マーケティング戦略などに関する場合にのみ使う。"
            "例:『マーケティング施策』『広告の効果』『集客方法』など。"
        )
    )

    wholecompany_theme_tool = Tool.from_function(
        func=run_wholecompany_theme_chain,
        name="全社会議議事録検索",
        description=(
            "質問内容が全社的な方針、経営戦略、会社全体の取り組み、全社イベント、"
            "経営層の発言、全社連絡事項などに関する場合にのみ使う。"
            "例:『経営方針』『全社イベント』『会社全体の目標』など。"
        )
    )

    sales_theme_tool = Tool.from_function(
        func=run_sales_theme_chain,
        name="営業議事録検索",
        description=(
            "質問内容が営業活動、売上、顧客開拓、商談、営業戦略、営業目標、"
            "営業会議などに関する場合にのみ使う。"
            "例:『営業戦略』『売上向上施策』『商談の進め方』など。"
        )
    )

    recruitment_theme_tool = Tool.from_function(
        func=run_recruitment_theme_chain,
        name="採用議事録検索",
        description=(
            "質問内容が採用活動、人材募集、面接、採用戦略、求人、採用プロセス、"
            "新卒・中途採用などに関する場合にのみ使う。"
            "例:『採用基準』『面接のポイント』『求人媒体』など。"
        )
    )

    education_theme_tool = Tool.from_function(
        func=run_education_theme_chain,
        name="教育・研修議事録検索",
        description=(
            "質問内容が社員教育、研修、育成、スキルアップ、OJT、社内勉強会、"
            "人材育成施策などに関する場合にのみ使う。"
            "例:『研修内容』『育成プラン』『教育施策』など。"
        )
    )

    development_theme_tool = Tool.from_function(
        func=run_development_theme_chain,
        name="開発議事録検索",
        description=(
            "質問内容がシステム開発、プロダクト開発、技術課題、開発プロセス、"
            "新機能、バグ対応、開発体制などに関する場合にのみ使う。"
            "例:『開発スケジュール』『技術課題』『新機能の要件』など。"
        )
    )

    client_theme_tool = Tool.from_function(
        func=run_client_theme_chain,
        name="顧客対応議事録検索",
        description=(
            "質問内容が顧客対応、クレーム、サポート、顧客満足、顧客要望、"
            "カスタマーサクセス、顧客とのやり取りなどに関する場合にのみ使う。"
            "例:『顧客要望』『クレーム対応』『サポート体制』など。"
        )
    )

    # LLMモデルの指定
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
    # 利用するToolをリスト化
    tools = [marketing_theme_tool, sales_theme_tool, recruitment_theme_tool, education_theme_tool, development_theme_tool, client_theme_tool]
    # Agent Executor作成
    agent_executor = initialize_agent(
        llm=llm,
        tools=tools,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True
    )

    # クエリの実行
    query = "自社メンバーの育成に関する具体的なアクションプランを教えて"
    # 変数「agent_executor」に格納されたエージェントにクエリを渡して実行
    agent_executor.run(query)

# --------------------------------------------------------------
# 5. エントリポイント
# --------------------------------------------------------------
if __name__ == "__main__":
    main()