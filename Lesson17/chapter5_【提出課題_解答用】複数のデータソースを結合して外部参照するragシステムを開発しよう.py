# -*- coding: utf-8 -*-
"""Chapter5:【提出課題: 解答用】複数のデータソースを結合して外部参照するRAGシステムを開発しよう

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rfiBZsCI7EOKDt3Dlq94YT5WBznY8IrH

# Lesson17: RAGシステムの開発に特化したLlamaIndex
# Chapter5:【提出課題: 解答用】複数のデータソースを結合して外部参照するRAGシステムを開発しよう

事前準備を行った上で、提出課題に取り組みましょう。

**【提出方法】**  
Slackでメンターをメンションの上、当シート右上の「共有」からリンクをコピーし、提出してください。

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""ライブラリのインストール"""

!pip install langchain==0.3.22 openai==1.70.0 langchain-community==0.3.20 httpx==0.28.1

"""## 提出課題

### 【問題文】

当Lessonの「Chapter2: LlamaIndexの基本」で扱った求人情報を使い、以下の条件で回答を生成するRAGシステムを作りましょう。


**【条件】**
*   回答生成のモデルには「gpt-3.5-turbo」を使ってください。
*   チャンクサイズを「500」、チャンク前後の重なりの文字数を「50」としてください。
*   データベースから関連情報として抽出するチャンク数の最大値を「3」としてください。
*   データベースが存在しない場合は新規作成してディスク上に保存し、すでにディスク上に保存されている場合は読み込んで使うように分岐処理を行ってください。
*   LlamaHubの「BeautifulSoupWebReader」を使い、実在する3つの求人ページの情報を読み込み、あらかじめ用意されている10個の求人情報と結合して、合計13個の求人情報をもとに回答が生成されるようにしてください。
*   作ったRAGシステムが期待通りに動作するか検証するためのユーザー入力を与え、回答を生成してください。

### 【解答】
"""

!pip install llama-index==0.12.7
!pip install llama-index-llms-langchain==0.5.0

from google.colab import drive
drive.mount('/content/drive')

source_folder = "/content/drive/MyDrive/Lesson17_data"

from llama_index.core import download_loader
from llama_index.core import GPTVectorStoreIndex
from llama_index.core import SimpleDirectoryReader
from llama_index.core import StorageContext, load_index_from_storage

# 指定したWebページからのデータ読み込み
BeautifulSoupWebReader = download_loader("BeautifulSoupWebReader")
loader = BeautifulSoupWebReader()
documents = loader.load_data(urls=[
    "https://generative-ai.web-camp.io/courses/marketing/",
    "https://generative-ai.web-camp.io/courses/sales/",
    "https://generative-ai.web-camp.io/courses/engineer/"
])

# ドキュメントをロード
pdf_documents = SimpleDirectoryReader(source_folder).load_data()

# すべてのドキュメントを結合
documents.extend(pdf_documents)

# データベースの作成
index = GPTVectorStoreIndex.from_documents(documents)
if not index.storage_context.persist(persist_dir="storage"):
    index.storage_context.persist()
else:
    storage_context = StorageContext.from_defaults(persist_dir="storage")
    index = load_index_from_storage(storage_context)

# クエリエンジンの作成
query_engine = index.as_query_engine()

# LLMからの回答取得（1回目）
print("\n----- LLMからの回答（1回目） -----")
result = query_engine.query("プロンプトエンジニアリングコースの「営業コース」は、どんな人におすすめですか？日本語で答えて")
print(result.response)

# 参照したドキュメントのパスを表示
for node in result.source_nodes:
    if "file_path" in node.metadata.keys():
        source_file_path = node.metadata["file_path"]
        print(source_file_path)

    if "URL" in node.metadata.keys():
        source_url = node.metadata["URL"]
        print(source_url)

# LLMからの回答取得（2回目）
print("\n----- LLMからの回答（2回目） -----")
result = query_engine.query("生成AIエンジニアコースの特徴を日本語で答えて")
print(result.response)

# 参照したドキュメントのパスを表示
for node in result.source_nodes:
    if "file_path" in node.metadata.keys():
        source_file_path = node.metadata["file_path"]
        print(source_file_path)

    if "URL" in node.metadata.keys():
        source_url = node.metadata["URL"]
        print(source_url)

# LLMからの回答取得（3回目）
print("\n----- LLMからの回答（3回目） -----")
result = query_engine.query("LangChainの開発経験を活かせる求人を教えて。日本語で回答して。")
print(result.response)

# 参照したドキュメントのパスを表示
for node in result.source_nodes:
    if "file_path" in node.metadata.keys():
        source_file_path = node.metadata["file_path"]
        print(source_file_path)

    if "URL" in node.metadata.keys():
        source_url = node.metadata["URL"]
        print(source_url)

