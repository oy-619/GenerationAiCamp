# -*- coding: utf-8 -*-
"""Chapter5:【演習問題: 解答用】Retrieval（RAG）

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVix8rQbT42ZKENHrDzyNwcmWv82Q5xI

# Lesson13: LangChainの主要モジュール4【Retrieval（RAG）】
# Chapter5:【演習問題: 解答用】Retrieval（RAG）

事前準備を行った上で、6つの演習問題に取り組みましょう。

各問題の「回答例/正解」と「解説」はデフォルトで非表示としていますが、  
非表示セルをクリックすれば確認できます。

まずは「回答例/正解」と「解説」を見ずにトライしてみましょう。

## 事前準備

シークレット機能でOpenAI APIのAPIキーを設定し、以下2つのコードを実行しましょう。
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2
# %pip install chromadb==0.5.11
# %pip install -qU langchain-community beautifulsoup4
# %pip install tiktoken==0.8.0
# %pip install pymupdf==1.24.11

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""「openai」ライブラリのログレベルを「DEBUG」にし、APIとのやり取りの詳細を確認できるようにしましょう。"""

import logging
logging.getLogger("openai").setLevel(logging.DEBUG)

"""## 演習問題

### 【問題1】
当コースの制作元であるDMM Groupの株式会社インフラトップが提供している「DMM 生成AICAMP」のサービス紹介ページ（ https://generative-ai.web-camp.io/ ）に掲載の内容を外部参照した上で、Webページの内容をもとに回答を生成するRAGシステムを作りましょう。

**【条件】**
*   質問内容は「コースの特徴を教えてください。」とします。
*   会話履歴の記憶機能を実装する必要はありません。
*   Webページの読み込みには「WebBaseLoader」を使いましょう。
*   ドキュメント分割には「CharacterTextSplitter」を使い、分割サイズは500文字、  
前後の文字の重なりは30文字、分割のための区切り文字は「\n」としましょう。
*   データベースをディスク上に保存する必要はありません。

**【注記】**

Webページの内容更新などにより、実行するタイミングによっては期待通りの回答を得られない可能性があります。  
その場合はWebページの内容を確認し、期待通りの回答を得られそうな質問を行ってください。
"""

# --------------------------------------------------
# 1.Document loaders
# --------------------------------------------------

# angchain_communityパッケージのdocument_loadersモジュールから、WebBaseLoaderクラスを読み込んでいる。
from langchain_community.document_loaders import WebBaseLoader
# WebBaseLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
loader = WebBaseLoader("https://generative-ai.web-camp.io/")
# 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
pages = loader.load()

pages[0].page_content

# --------------------------------------------------
# 3.Text embedding models
# --------------------------------------------------
# OpenAIEmbeddingsクラスを読み込み、インスタンスを作る
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# --------------------------------------------------
# 4.Vector Stores
# --------------------------------------------------
# 「vectorstores」モジュールからChroma（クロマ）クラスを読み込む
from langchain.vectorstores import Chroma
# 「from_documents()」メソッドを使ってインスタンス化
# メソッドの引数には以下2つのデータを渡している。
# 1.Documentオブジェクトを要素に持つリスト（変数「splitted_pages」）
# 2.テキストをベクトル化するためのOpenAIEmbeddingsクラスのインスタンス（変数「embeddings」）
# コードを実行することで、内部的にドキュメント内のテキストがベクトル化され、
# ベクトル化されたテキストがChromaのデータベースに保存される。
db = Chroma.from_documents(pages, embedding=embeddings)

# --------------------------------------------------
# 5.RetrievalQA
# --------------------------------------------------
# ChatOpenAIクラスに加え、今回のコードの主役である「RetrievalQA」クラスを読み込む
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

retriever = db.as_retriever()

# ChatOpenAIクラスのインスタンスを用意した後、
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
# RetrievalQAクラスの「from_chain_type()」メソッドを使い、RetrievalQAクラスのインスタンスを作っている
# from_chain_type()メソッドには、「llm」引数でLLMにリクエストを送るためのモデルのインスタンスを、
# また「retriever」引数で、参照した外部情報のドキュメントが格納されているVector Storesから検索を行うためのRetrieverのインスタンスを指定
# 「chain_type」には「stuff」を指定(取得した複数のドキュメントを全てプロンプトに埋め込む指定)
# 他にも「map_reduce」「map_rerank」「refine」という値を指定できる。(何の値を指定するかによって消費トークン数や回答精度が変わる)
# stuff：取得したドキュメントを全てプロンプトに埋め込み、LLMに渡す。
# map_reduce：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、最終的に各回答結果を結合してLLMに渡す。
# map_rerank：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、各回答結果に点数を付けて一番高かったものを最終的な回答とする。
# refine：1つ目のチャンクをプロンプトに埋め込んでLLMに渡し、回答結果を次のチャンクのプロンプトと結合し、最後のチャンクまでこの処理を繰り返す。
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# RetrievalQAクラスのインスタンス「chain」に対して「run()」メソッドを呼び出し、
# 引数に入力内容を渡すことで、内部的に以下の処理が実行される
query = "DMM 生成AI CAMPの受講の注意点を教えてください。"

# 1.入力したテキストにベクトル化の処理が施され、数値のリストに変換される。
# 2.ベクトル化済みのPDFファイルのテキストが格納されたベクターストアから、入力内容と関連性が高いドキュメントを探し、取得する。
# 3.取得したドキュメントがプロンプトに埋め込まれる。
# 4.プロンプトがLLMに渡り、回答が生成される。
result = chain.run(query)
print(result)

"""#### 【解答例】"""

from langchain_community.document_loaders import WebBaseLoader

url = "https://generative-ai.web-camp.io/"
loader = WebBaseLoader(url)
web_doc = loader.load()

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=30,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(web_doc)

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

!pip install chromadb==0.5.11 tiktoken==0.8.0

from langchain.vectorstores import Chroma

db = Chroma.from_documents(web_doc, embedding=embeddings)

retriever = db.as_retriever()

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

query = "コースの特徴を教えてください。"

result = chain.run(query)
print(result)

"""#### 【解説】

Document loadersとして「PyMuPDFLoader」ではなく「WebBaseLoader」を使うところ以外は、  
教材で解説したコードと基本的には同じです。

### 【問題2】
「問題1」の実行結果のログを確認し、LLMに渡すプロンプトの中からWebページの内容埋め込みの前に自動的に入る英語のシステムメッセージを探し、以下の解答欄に転記してください。

解答欄："Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer

#### 【解答例】

Use the following pieces of context to answer the user's question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.

#### 【解説】

実行結果のログで「'url': '/chat/completions'」が記載されている、Chat Completions APIのリクエストに関する情報が記載されている行に、LLMに渡されるプロンプトが含まれています。その中に、自動的に入る英語のシステムメッセージが記載されています。

### 【問題3】
Webページとして公開されている「会社法」のPDFファイル内のテキストを外部参照し、  
会社法の内容に応じて質問に回答するRAGシステムを作りましょう。

「会社法」のページURL:  
https://laws.e-gov.go.jp/data/Act/417AC0000000086/618544_1/417AC0000000086_20240522_506AC0000000032_h1.pdf

**【条件】**
*   質問内容は「会社設立の際に、特に注意するべき点について500文字以内で回答してください。」とします。
*   会話履歴の記憶機能を実装する必要はありません。
*   PDFファイルの読み込みには「PyMuPDFLoader」を使いましょう。
*   ドキュメント分割には「CharacterTextSplitter」を使い、分割サイズは1000文字、  
前後の文字の重なりは100文字、分割のための区切り文字は「\n」としましょう。
*   データベースを保存する必要はありません。
"""

# --------------------------------------------------
# 1.Document loaders
# --------------------------------------------------

# angchain_communityパッケージのdocument_loadersモジュールから、PyMuPDFLoaderクラスを読み込んでいる。
from langchain_community.document_loaders import PyMuPDFLoader
# PyMuPDFLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
loader = PyMuPDFLoader("https://laws.e-gov.go.jp/data/Act/417AC0000000086/618544_1/417AC0000000086_20240522_506AC0000000032_h1.pdf")
# 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
pages = loader.load()
# len関数でリストの要素数を確認
len(pages)

# --------------------------------------------------
# 2.Document transformers
# --------------------------------------------------
# ext_splitterモジュールからCharacterTextSplitterクラスを読み込み、インスタンス化
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
    chunk_size=200,
    # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
    chunk_overlap=50,
    # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
    separator="\n",
)
# CharacterTextSplitterのインスタンスに対して「split_documents()」メソッドを呼び出し、
# 引数に読み込んだドキュメント（リストの各要素がPDFファイルの1ページ分のテキストであるもの）を指定
splitted_pages = text_splitter.split_documents(pages)

len(splitted_pages)

# --------------------------------------------------
# 3.Text embedding models
# --------------------------------------------------
# OpenAIEmbeddingsクラスを読み込み、インスタンスを作る
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# --------------------------------------------------
# 4.Vector Stores
# --------------------------------------------------
# 「vectorstores」モジュールからChroma（クロマ）クラスを読み込む
from langchain.vectorstores import Chroma
# 「from_documents()」メソッドを使ってインスタンス化
# メソッドの引数には以下2つのデータを渡している。
# 1.Documentオブジェクトを要素に持つリスト（変数「splitted_pages」）
# 2.テキストをベクトル化するためのOpenAIEmbeddingsクラスのインスタンス（変数「embeddings」）
# コードを実行することで、内部的にドキュメント内のテキストがベクトル化され、
# ベクトル化されたテキストがChromaのデータベースに保存される。
# 実行ファイルと同じフォルダ内に「.db」という名前のフォルダが作成される。
db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".company_law_db")

# --------------------------------------------------
# 5.RetrievalQA
# --------------------------------------------------
# ChatOpenAIクラスに加え、今回のコードの主役である「RetrievalQA」クラスを読み込む
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

retriever = db.as_retriever()

# ChatOpenAIクラスのインスタンスを用意した後、
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
# RetrievalQAクラスの「from_chain_type()」メソッドを使い、RetrievalQAクラスのインスタンスを作っている
# from_chain_type()メソッドには、「llm」引数でLLMにリクエストを送るためのモデルのインスタンスを、
# また「retriever」引数で、参照した外部情報のドキュメントが格納されているVector Storesから検索を行うためのRetrieverのインスタンスを指定
# 「chain_type」には「stuff」を指定(取得した複数のドキュメントを全てプロンプトに埋め込む指定)
# 他にも「map_reduce」「map_rerank」「refine」という値を指定できる。(何の値を指定するかによって消費トークン数や回答精度が変わる)
# stuff：取得したドキュメントを全てプロンプトに埋め込み、LLMに渡す。
# map_reduce：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、最終的に各回答結果を結合してLLMに渡す。
# map_rerank：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、各回答結果に点数を付けて一番高かったものを最終的な回答とする。
# refine：1つ目のチャンクをプロンプトに埋め込んでLLMに渡し、回答結果を次のチャンクのプロンプトと結合し、最後のチャンクまでこの処理を繰り返す。
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# RetrievalQAクラスのインスタンス「chain」に対して「run()」メソッドを呼び出し、
# 引数に入力内容を渡すことで、内部的に以下の処理が実行される
query = "会社設立の際に、特に注意するべき点について500文字以内で回答してください。"

# 1.入力したテキストにベクトル化の処理が施され、数値のリストに変換される。
# 2.ベクトル化済みのPDFファイルのテキストが格納されたベクターストアから、入力内容と関連性が高いドキュメントを探し、取得する。
# 3.取得したドキュメントがプロンプトに埋め込まれる。
# 4.プロンプトがLLMに渡り、回答が生成される。
result = chain.run(query)
print(result)

"""#### 【解答例】"""

!pip install pymupdf==1.24.11

from langchain_community.document_loaders import PyMuPDFLoader

url = "https://laws.e-gov.go.jp/data/Act/417AC0000000086/618544_1/417AC0000000086_20240522_506AC0000000032_h1.pdf"
loader = PyMuPDFLoader(url)
pages = loader.load()

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(pages)

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

!pip install chromadb==0.5.11 tiktoken==0.8.0

from langchain.vectorstores import Chroma

db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".company_law_db")

retriever = db.as_retriever()

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

query = "会社設立の際に、特に注意するべき点について500文字以内で回答してください。"

result = chain.run(query)
print(result)

"""#### 【解説】

Webに公開されている場合でも、PDF形式であれば「PyMuPDFLoader」でデータを読み込めます。

「会社法」のような大容量データでも、読み込んでデータベースに格納し、入力内容と関連性の高いドキュメントを抽出してプロンプトに含め、LLMに回答を生成させることができます。

### 【問題4】
「問題3」で作成したRAGシステムは、外部参照するデータが大容量でした。このような大容量データの外部参照においては、一度読み込んだデータをデータベースに保存しておき、後から読み込んで使えるようにするのが良いです。

すでにデータベースが存在する場合は読み込み、存在しない場合は新規作成して保存するよう、「問題3」のコードを書き換えましょう。

**【条件】**

データベース名は「.company_law_db」とします。


**【注記】**

「問題3」で作成したコード、もしくは解答例のコードをコピーして問題に取り掛かってください。

#### 【ヒント】

「os」モジュールを読み込み、if文の条件式に「os.path.isdir(DB名)」と書くことで

データベースの存在有無を確認でき、分岐処理を行えます。

#### 【解答】
"""





















"""#### 【解答例】"""

!pip install pymupdf==1.24.11

from langchain_community.document_loaders import PyMuPDFLoader

url = "https://laws.e-gov.go.jp/data/Act/417AC0000000086/618544_1/417AC0000000086_20240522_506AC0000000032_h1.pdf"
loader = PyMuPDFLoader(url)
pages = loader.load()

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(pages)

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

!pip install chromadb==0.5.11 tiktoken==0.8.0

from langchain.vectorstores import Chroma

if os.path.isdir(".company_law_db"):
    db = Chroma(persist_directory=".company_law_db", embedding_function=embeddings)
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".company_law_db")

retriever = db.as_retriever()

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

query = "会社設立の際に、特に注意するべき点について500文字以内で回答してください。"

result = chain.run(query)
print(result)

"""#### 【解説】

「os」モジュールを読み込み、if文の条件式に「os.path.isdir(DB名)」と書くことで、  
データベースが存在する場合としない場合で処理を分岐させられます。

データベースが存在する場合は読み込み処理を行い、存在しない場合は新規作成・保存の処理を行います。

### 【問題5】
**【事前準備】**  
サイドバーのファイルエリア上で「data」フォルダを作り、フォルダ内に以下3つのファイルをアップロードしましょう。  
※まずはGoogleドライブからダウンロードする必要があります。

**ファイルが保存されているフォルダ**

「Lesson13: LangChainの主要モジュール4【Retrieval（RAG）】」フォルダ  
└「演習問題/提出課題」フォルダ  
└└「演習問題用データ」フォルダ

**ファイル**

*   healthX_instructions.pdf（サービス詳細）
*   healthX_制作及び保守業務仕様書.pdf
*   20241207_MTG議事録_healthXのマーケティング施策について.pdf


**【問題文】**  
「data」フォルダ内の全ファイルを外部参照先として、ユーザーからの質問に回答するRAGシステムを作りましょう。

**【条件】**
*   質問内容は「HealthXの無料プランについて教えてください。」とします。
*   会話履歴の記憶機能を実装する必要はありません。
*   すでにデータベースが存在する場合は読み込み、存在しない場合は新規作成して保存しましょう。
*   データベース名は「.healthX_db」とします。
*   ドキュメント分割には「CharacterTextSplitter」を使い、分割サイズは500文字、  
前後の文字の重なりは30文字、分割のための区切り文字は「\n」としましょう。
*   「os.listdir(フォルダ名)」を使い、フォルダ内の各ファイル名を要素に持つリストを取得した後、  
各ファイルに対してfor文で順にドキュメントを読み込みましょう。

#### 【ヒント】

*   Google Colaboratory上でフォルダを作ると、フォルダ内に「.ipynb_checkpoints」という名前の隠しファイルが自動で作られます。  
このファイルについてはfor文の中で「continue」を使って処理をスキップしましょう。
*   「extend()」メソッドを使うことで、各ページを要素に持つファイル単位のリスト同士を結合できます。

#### 【解答】
"""

# --------------------------------------------------
# 1.Document loaders
# --------------------------------------------------

# angchain_communityパッケージのdocument_loadersモジュールから、PyMuPDFLoaderクラスを読み込んでいる。
from langchain_community.document_loaders import PyMuPDFLoader
import os

pages = []

abs_path = os.path.abspath('.')
dir_name = 'data'
path = os.path.join(abs_path, dir_name)
# フォルダ内の各ファイル名を要素に持つリストを取得
doc_list = os.listdir("/content/data")
# 各ファイルに対してfor文で順にドキュメントを読み込み
for indx, doc in enumerate(doc_list):
  doc_paths = os.path.join(path, doc)
  # PyMuPDFLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
  loader = PyMuPDFLoader(doc_paths)
  # 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
  page = loader.load()
  # リスト同士を結合
  pages.extend(page)

  # len関数でリストの要素数を確認
  print(pages[0].page_content)

# --------------------------------------------------
# 2.Document transformers
# --------------------------------------------------
# ext_splitterモジュールからCharacterTextSplitterクラスを読み込み、インスタンス化
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
    chunk_size=500,
    # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
    chunk_overlap=30,
    # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
    separator="\n",
)
# CharacterTextSplitterのインスタンスに対して「split_documents()」メソッドを呼び出し、
# 引数に読み込んだドキュメント（リストの各要素がPDFファイルの1ページ分のテキストであるもの）を指定
splitted_pages = text_splitter.split_documents(pages)

len(splitted_pages)

# --------------------------------------------------
# 3.Text embedding models
# --------------------------------------------------
# OpenAIEmbeddingsクラスを読み込み、インスタンスを作る
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# --------------------------------------------------
# 4.Vector Stores
# --------------------------------------------------
# 「vectorstores」モジュールからChroma（クロマ）クラスを読み込む
from langchain.vectorstores import Chroma
# 「from_documents()」メソッドを使ってインスタンス化
# メソッドの引数には以下2つのデータを渡している。
# 1.Documentオブジェクトを要素に持つリスト（変数「splitted_pages」）
# 2.テキストをベクトル化するためのOpenAIEmbeddingsクラスのインスタンス（変数「embeddings」）
# コードを実行することで、内部的にドキュメント内のテキストがベクトル化され、
# ベクトル化されたテキストがChromaのデータベースに保存される。
# 実行ファイルと同じフォルダ内に「.healthX_db」という名前のフォルダが作成される。
if os.path.isdir(".healthX_db_1"):
    db = Chroma(persist_directory=".healthX_db_1", embedding_function=embeddings)
    # 新しい文書を足すときは add_documents を使う
    db.add_documents(splitted_pages)
    db.persist()
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".healthX_db_1")
    db.persist()

# --------------------------------------------------
# 5.RetrievalQA
# --------------------------------------------------
# ChatOpenAIクラスに加え、今回のコードの主役である「RetrievalQA」クラスを読み込む
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

retriever = db.as_retriever()

# ChatOpenAIクラスのインスタンスを用意した後、
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
# RetrievalQAクラスの「from_chain_type()」メソッドを使い、RetrievalQAクラスのインスタンスを作っている
# from_chain_type()メソッドには、「llm」引数でLLMにリクエストを送るためのモデルのインスタンスを、
# また「retriever」引数で、参照した外部情報のドキュメントが格納されているVector Storesから検索を行うためのRetrieverのインスタンスを指定
# 「chain_type」には「stuff」を指定(取得した複数のドキュメントを全てプロンプトに埋め込む指定)
# 他にも「map_reduce」「map_rerank」「refine」という値を指定できる。(何の値を指定するかによって消費トークン数や回答精度が変わる)
# stuff：取得したドキュメントを全てプロンプトに埋め込み、LLMに渡す。
# map_reduce：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、最終的に各回答結果を結合してLLMに渡す。
# map_rerank：チャンクの数だけ別々でプロンプトに埋め込んだものをそれぞれLLMに渡し、各回答結果に点数を付けて一番高かったものを最終的な回答とする。
# refine：1つ目のチャンクをプロンプトに埋め込んでLLMに渡し、回答結果を次のチャンクのプロンプトと結合し、最後のチャンクまでこの処理を繰り返す。
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# RetrievalQAクラスのインスタンス「chain」に対して「run()」メソッドを呼び出し、
# 引数に入力内容を渡すことで、内部的に以下の処理が実行される
query = "HealthXの無料プランについて教えてください。"

# 1.入力したテキストにベクトル化の処理が施され、数値のリストに変換される。
# 2.ベクトル化済みのPDFファイルのテキストが格納されたベクターストアから、入力内容と関連性が高いドキュメントを探し、取得する。
# 3.取得したドキュメントがプロンプトに埋め込まれる。
# 4.プロンプトがLLMに渡り、回答が生成される。
result = chain.run(query)
print(result)

"""#### 【解答例】"""

!pip install pymupdf==1.24.11

from langchain_community.document_loaders import PyMuPDFLoader
import os

folder_name = "data"
files = os.listdir(folder_name)

docs = []
for file in files:
    if not file.endswith(".pdf"):
        continue
    loader = PyMuPDFLoader(f"{folder_name}/{file}")
    pages = loader.load()
    docs.extend(pages)

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=30,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(docs)

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

!pip install chromadb==0.5.11 tiktoken==0.8.0

from langchain.vectorstores import Chroma

if os.path.isdir(".healthX_db"):
    db = Chroma(persist_directory=".healthX_db", embedding_function=embeddings)
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".healthX_db")

retriever = db.as_retriever()

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

query = "HealthXの無料プランについて教えてください。"

result = chain.run(query)
print(result)

"""#### 【解説】

「os.listdir(フォルダ名)」と書くことで、指定したフォルダ内の各ファイル名を要素に持つリストを取得できます。

for文でリストをループ処理し、各ファイルを「PyMuPDFLoader」で読み込んだ後、  
for文の外で用意したリストに順次「extend()」メソッドで結合していくことにより、  
フォルダ内のすべてのファイルを1つのリストに集約できます。

### 【問題6】
「Chapter4: RAGシステムに会話履歴の記憶機能を実装する方法」を参照し、  
「問題5」で作成したRAGシステムに会話履歴の記憶機能を実装しましょう。

会話履歴の記憶機能が実装できていることを確認するため、LLM呼び出しを2回行い、  
1回目の呼び出しでは「HealthXの無料プランについて教えてください。」、  
2回目の呼び出しでは「より詳しく回答してください。」と入力を与えてください。

**【注記】**

「問題5」で作成したコード、もしくは解答例のコードをコピーして問題に取り掛かってください。
"""

# --------------------------------------------------
# 1.Document loaders
# --------------------------------------------------

# angchain_communityパッケージのdocument_loadersモジュールから、PyMuPDFLoaderクラスを読み込んでいる。
from langchain_community.document_loaders import PyMuPDFLoader
import os

pages = []

abs_path = os.path.abspath('.')
dir_name = 'data'
path = os.path.join(abs_path, dir_name)
# フォルダ内の各ファイル名を要素に持つリストを取得
doc_list = os.listdir("/content/data")
# 各ファイルに対してfor文で順にドキュメントを読み込み
for indx, doc in enumerate(doc_list):
  doc_paths = os.path.join(path, doc)
  # PyMuPDFLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
  loader = PyMuPDFLoader(doc_paths)
  # 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
  page = loader.load()
  # リスト同士を結合
  pages.extend(page)

  # len関数でリストの要素数を確認
  print(pages[0].page_content)

# --------------------------------------------------
# 2.Document transformers
# --------------------------------------------------
# ext_splitterモジュールからCharacterTextSplitterクラスを読み込み、インスタンス化
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
    chunk_size=500,
    # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
    chunk_overlap=30,
    # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
    separator="\n",
)
# CharacterTextSplitterのインスタンスに対して「split_documents()」メソッドを呼び出し、
# 引数に読み込んだドキュメント（リストの各要素がPDFファイルの1ページ分のテキストであるもの）を指定
splitted_pages = text_splitter.split_documents(pages)

len(splitted_pages)

# --------------------------------------------------
# 3.Text embedding models
# --------------------------------------------------
# OpenAIEmbeddingsクラスを読み込み、インスタンスを作る
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# --------------------------------------------------
# 4.Vector Stores
# --------------------------------------------------
# 「vectorstores」モジュールからChroma（クロマ）クラスを読み込む
from langchain.vectorstores import Chroma
# 「from_documents()」メソッドを使ってインスタンス化
# メソッドの引数には以下2つのデータを渡している。
# 1.Documentオブジェクトを要素に持つリスト（変数「splitted_pages」）
# 2.テキストをベクトル化するためのOpenAIEmbeddingsクラスのインスタンス（変数「embeddings」）
# コードを実行することで、内部的にドキュメント内のテキストがベクトル化され、
# ベクトル化されたテキストがChromaのデータベースに保存される。
# 実行ファイルと同じフォルダ内に「.healthX_db」という名前のフォルダが作成される。
if os.path.isdir(".healthX_db_1"):
    db = Chroma(persist_directory=".healthX_db_1", embedding_function=embeddings)
    # 新しい文書を足すときは add_documents を使う
    db.add_documents(splitted_pages)
    db.persist()
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".healthX_db_1")
    db.persist()

# -------------------------------------------------------------
# 「外部情報の参照」と「会話履歴の記憶」を同時に実現する仕組み
# 1.LLMに最新の入力内容と会話履歴を渡し、会話履歴なしでも理解できるLLMへの独立した（新しい）入力内容を生成する（1回目のLLMリクエスト）。
# 2.生成された入力内容をもとに、ベクターストアから関連するドキュメントを取得する。
# 3.生成された入力内容と、ベクターストアから取得した関連ドキュメントをプロンプトに埋め込み、LLMに渡して回答を生成する（2回目のLLMリクエスト）。
# 4.Chains内ではLLMへのリクエストが2回行われます。特に注目するべきは、1回目のリクエストです。
# -------------------------------------------------------------

# これらの処理を実現するために、以下3つのインスタンスを順に用意します。
# 1.会話履歴なしでも理解できる独立した入力内容を生成し、これをもとにベクターストアから適切な関連ドキュメントを取得する「create_history_aware_retriever」のインスタンス（手順1と2に対応）
# 2.生成した入力内容と取得した関連ドキュメントをもとに、LLMに回答を生成させる「create_stuff_documents_chain」のインスタンス（手順3に対応）
# 3.これら2つのインスタンスをまとめて、手順1〜3を連鎖的に実行する機能を持つ「create_retrieval_chain」のインスタンス

# ***********************************************************
# 1. create_history_aware_retriever（手順1と2に対応）
# ***********************************************************
# 会話履歴の記憶機能を実装するにあたって必要なクラスや関数などを一式読み込んでおく。
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
# 「create_history_aware_retriever」は、「手順1: 独立した入力内容の生成」と
# 「手順2: 生成した入力内容をもとにドキュメントを取得」を行える。
# 「create_retrieval_chain」は、「create_history_aware_retriever」と
# 「create_stuff_documents_chain」をまとめるChain。
# 最終的にこのクラスの機能を使うことで、手順1〜3の処理を実行
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
#「create_stuff_documents_chain」は、「手順3: 独立した入力内容と
# 関連ドキュメントをもとに回答を生成」を行える。
from langchain.chains.combine_documents import create_stuff_documents_chain

# 手順1〜3の処理を実現するにあたり、LLMへのリクエストは以下の2回行われる。
# 1.会話履歴がなくても理解できる、独立した入力を生成するためのLLMリクエスト
# 2.生成された入力内容と関連ドキュメントを渡して、最終的な回答を生成するためのLLMリクエスト
# ここでは「1. 会話履歴がなくても理解できる、独立した入力を生成するためのLLMリクエスト」を行うための、専用のプロンプトを用意。
question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"

# ChatPromptTemplateでは、LLMの振る舞いを制御するシステムメッセージとユーザーメッセージ、
# また会話履歴を差し込むためのプレースホルダーを用意している。
# システムメッセージとユーザーメッセージは、このように省略した書き方が可能。
question_generator_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_generator_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),

    ]
)
# 呼び出すLLMのインスタンスを用意。
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# 呼び出すLLMと、ベクターストア検索のためのRetriever、
# また独立した入力生成用のプロンプトを渡すことで
# 「create_history_aware_retriever」のインスタンスを生成。
# Retrieverには、「Retrievers」の前パートで作成したインスタンス
# (retriever = db.as_retriever())を使う。
# これで、手順1と2を実行する準備が完了。
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, question_generator_prompt
)

# ***********************************************************
# 2. create_stuff_documents_chain（手順3に対応）
# ***********************************************************
# 会話履歴なしでも理解できる独立した入力内容と、
# ベクターストアから取得した関連ドキュメントをもとに
# LLMから回答を得るためのプロンプトを用意。
# 「{context}」の箇所に関連ドキュメントが埋め込まれる。
# このプロンプトを使うことで、入力内容に対して会話履歴を踏まえた回答を得られる。
question_answer_template = """
あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"
{context}
"""
question_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_answer_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 呼び出すLLMとプロンプトを引数として渡し
# 「create_stuff_documents_chain」のインスタンスを生成。
# このインスタンスの機能を使うことで、会話履歴なしでも理解できる
# 独立した入力内容と取得した関連ドキュメントをもとに、LLMに回答を生成させることができる。
question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)

# ***********************************************************
# 3. create_retrieval_chain（1と2のインスタンスをまとめたChains（手順1〜3に対応））
# ***********************************************************
# 1つ目に作った「create_history_aware_retriever」のインスタンスを使うことで、
# 会話履歴なしでも理解できる独立した入力内容の生成と、適した関連ドキュメントの取得を行える。
# そして2つ目に作った「create_stuff_documents_chain」のインスタンスを使うことで、
# 最終的にLLMから回答を得られる。
# これら2つのインスタンスの機能をまとめたChainsが、「create_retrieval_chain」。
# このChainsを実行することで、「外部情報の参照」と「会話履歴の記憶」を
# 同時に実現するための一連の機能を連鎖的に実行できる。

# 引数には、先ほど作成した「create_history_aware_retriever」のインスタンスと、
# 「create_stuff_documents_chain」のインスタンスを渡す。
# 後ほど、この「create_retrieval_chain」のインスタンスが持つ「invoke()」メソッドに
# 「入力内容」と「会話履歴」の2つのデータを渡すことで、独立した入力内容の生成と
# 関連ドキュメントの取得、最終的なLLMからの回答生成を内部的に一括で行える。
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# 「invoke()」メソッドを実行してLLMに回答を生成させる前に、以下のコードを実行しておく。
# 「openai」ライブラリのログレベルを「DEBUG」に設定している。
# これによりopenaiライブラリが使われた際の内部動作が、
# プログラムの実行結果として詳細に表示されるようになる。
import logging
logging.getLogger("openai").setLevel(logging.DEBUG)

# LLM呼び出しを行う前に、会話履歴を保持するためのデータの入れ物を用意。
# 2回目以降のLLM呼び出しでは、入力内容と会話履歴をもとに、
# 会話履歴なしでもLLMが理解できる「独立した入力内容」を生成する。
# そのため入力内容とLLMからの回答内容は、LLM呼び出しのたびに
# 会話履歴として保存していく必要がある。
chat_history = []

# 入力内容を変数「query」として用意した後、
# create_retrieval_chainのインスタンス「rag_chain」に対して
# 「invoke()」メソッドを実行している。引数には入力内容と会話履歴を渡す。
query = "HealthXの無料プランについて教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})

print(ai_msg["answer"])

# 「extend()」は、リスト同士を結合するメソッド。
# 入力内容とLLMからの回答内容を要素に持つリストを渡すことで、会話履歴が更新される。
chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

query = "もう少し詳しく教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})

print(ai_msg["answer"])

chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

"""#### 【解答例】"""

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

!pip install pymupdf==1.24.11

from langchain_community.document_loaders import PyMuPDFLoader
import os

folder_name = "data"
files = os.listdir(folder_name)

docs = []
for file in files:
    if not file.endswith(".pdf"):
        continue
    loader = PyMuPDFLoader(f"{folder_name}/{file}")
    pages = loader.load()
    docs.extend(pages)

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=30,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(docs)

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

!pip install chromadb==0.5.11 tiktoken==0.8.0

from langchain.vectorstores import Chroma

if os.path.isdir(".healthX_db"):
    db = Chroma(persist_directory=".healthX_db", embedding_function=embeddings)
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".healthX_db")

retriever = db.as_retriever()

question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"

question_generator_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_generator_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, question_generator_prompt
)

question_answer_template = """
あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"

{context}
"""

question_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_answer_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

chat_history = []

query = "HealthXの無料プランについて教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})
print(ai_msg["answer"])
chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

query = "もう少し詳しく教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})
print(ai_msg["answer"])
chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

"""#### 【解説】

以下3つの手順を踏むことで、RAGシステムに会話履歴の記憶機能を実装できます。

1.   会話履歴なしでもLLMが理解できる、独立した入力内容を生成する。
2.   独立した入力内容をもとに、ベクターストから関連性の高いドキュメントを取得する。
3.   独立した入力内容と関連ドキュメントをもとに、LLMに回答を生成させる。

これら3つの手順を踏む具体的な方法として、以下3つのインスタンスを作成します。

1.   create_history_aware_retriever: 手順1と2の実行準備
2.   create_stuff_documents_chain: 手順3の実行準備
3.   create_retrieval_chain: 手順1〜3の実行

「Chapter4: RAGシステムに会話履歴の記憶機能を実装する方法」で解説した内容を理解できれば、  
会話履歴の機能を搭載したRAGシステムを難なく作れるようになります。
"""