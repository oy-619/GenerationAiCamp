# -*- coding: utf-8 -*-
"""【教材サンプルコード】Lesson13: Retrieval

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YJhUFtkixLnvTilwm6WxjKaW4-dEJpTo

# 【教材ソースコード】Lesson13: LangChainの主要モジュール4【Retrieval（RAG）】

## 事前準備
シークレット機能でOpenAI APIのAPIキーを設定し、以下2つのコードを実行しましょう。
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2

"""## 教材サンプルコード

### Chapter2: Retrieval（RAG）の概要

---
"""

from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="今の日本の総理大臣は誰ですか？")
]

result = llm(messages)
print(result.content)

"""### Chapter3: Retrievalの基本的な使い方

---

#### Document loaders

「pymupdf」パッケージのインストール
"""

!pip install pymupdf==1.24.11

"""PDFファイルの読み込み"""

from langchain_community.document_loaders import PyMuPDFLoader

loader = PyMuPDFLoader("healthX_instructions.pdf")
pages = loader.load()

len(pages)

"""PDFファイルの1ページ目の内容確認"""

pages[0]

"""「page_content」属性にテキストが格納されている"""

pages[0].page_content

"""#### Document transformers

現在のドキュメントの要素数を確認
"""

len(pages[0].page_content)

"""「WebBaseLoader」を使ってWebページの情報を取得"""

from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://generative-ai.web-camp.io/")
web_docs = loader.load()

"""Webページからデータを読み込んだ場合、テキストの一つの塊が大きいことが多い"""

len(web_docs[0].page_content)

"""ドキュメントの分割"""

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=30,
    separator="\n",
)

splitted_pages = text_splitter.split_documents(pages)
len(splitted_pages)

"""#### Text embedding models

「OpenAIEmbeddings」クラスのインスタンスを作成
"""

from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

"""テキストのベクトル化を行うために「tiktoken」パッケージをインストール"""

!pip install tiktoken==0.8.0

"""テキストのベクトル化"""

vector_data = embeddings.embed_query("犬")
print(vector_data)

"""ベクトル化後のリストの要素数を確認"""

len(vector_data)

"""#### Vector Stores

「chroma」パッケージのインストール
"""

!pip install chromadb==0.5.11

"""データベースにドキュメントを保存"""

from langchain.vectorstores import Chroma

db = Chroma.from_documents(splitted_pages, embedding=embeddings)

"""データベースを保存したい場合は「persist_directory」を指定する"""

db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".db")

"""保存したデータベースを読み込む際のコード"""

db = Chroma(persist_directory=".db", embedding_function=embeddings)

"""#### Retrievers

Retrieverの作成
"""

retriever = db.as_retriever()

"""入力内容と関連性の高いドキュメントを取得"""

query = "HealthXの、ユーザー体験向上のための仕掛けを教えてください。"

docs = retriever.get_relevant_documents(query)
print(len(docs))

"""取得したドキュメントの確認"""

print(docs[1].page_content)

"""#### RetrievalQA

RetrievalQAのChainsを使得ことで、入力内容のベクトル化とデータベースからの関連ドキュメントの抽出、  
プロンプトの作成、LLMからの回答取得を一括で行える
"""

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

query = "HealthXの、ユーザー体験向上のための仕掛けを一言で教えてください。"

result = chain.run(query)
print(result)

"""### Chapter4: Retrievalと会話履歴の保持

---

#### 「Memory」のおさらい
"""

from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory()

chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

chain.predict(input="生成AIの「RAG」という技術について300文字以内で教えてください。")

"""LLMが会話履歴を踏まえた回答をしてくれていることを確認"""

chain.predict(input="100文字以内で要約してください。")

"""#### RetrievalQAでは、会話履歴を踏まえた回答生成ができない"""

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

memory = ConversationBufferMemory()

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    memory=memory
)

chain.run("HealthXの、ユーザー体験向上のための仕掛けを一言で教えてください。")

chain.run("もう少し詳しく教えてください。")

"""#### RAGシステムに会話履歴の記憶機能を実装するコードの解説

会話履歴の記憶機能を実装するにあたって必要なクラスや関数などを一式読み込む
"""

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

"""「会話履歴がなくても理解できる、独立した入力を生成するためのLLMリクエスト」を行うための、専用のプロンプトを用意"""

question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"

question_generator_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_generator_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

"""呼び出すLLMのインスタンスを用意"""

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

"""「create_history_aware_retriever」のインスタンスを作成"""

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, question_generator_prompt
)

"""会話履歴なしでも理解できる独立した入力内容と、ベクターストアから取得した  
関連ドキュメントをもとにLLMから回答を得るためのプロンプトを用意
"""

question_answer_template = """
あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"

{context}
"""

question_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_answer_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

"""「create_stuff_documents_chain」のインスタンスを作成"""

question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)

"""「create_retrieval_chain」のインスタンスを作成"""

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

"""「openai」ライブラリのログレベルを「DEBUG」に設定"""

import logging
logging.getLogger("openai").setLevel(logging.DEBUG)

"""会話履歴を保持するためのデータの入れ物を用意"""

chat_history = []

"""1回目のLLM呼び出し"""

query = "HealthXの、ユーザー体験向上のための仕掛けを教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})
print(ai_msg["answer"])

"""会話履歴の更新"""

chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

"""2回目のLLM呼び出し（会話履歴を踏まえてLLMが回答してくれることを確認）"""

query = "もう少し詳しく教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})
print(ai_msg["answer"])
chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])