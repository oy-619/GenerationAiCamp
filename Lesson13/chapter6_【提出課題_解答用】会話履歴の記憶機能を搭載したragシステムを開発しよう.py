# -*- coding: utf-8 -*-
"""Chapter6:【提出課題: 解答用】会話履歴の記憶機能を搭載したRAGシステムを開発しよう

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xPooRIWnSUOL8vAe81XbBCqRftZZ-W6b

# Lesson13: LangChainの主要モジュール4【Retrieval（RAG）】
# Chapter6:【提出課題: 解答用】会話履歴の記憶機能を搭載したRAGシステムを開発しよう

事前準備を行った上で、提出課題に取り組みましょう。

**【提出方法】**  
Slackでメンターをメンションの上、当シート右上の「共有」からリンクをコピーし、提出してください。

## 事前準備

### 【OpenAI APIキーの設定】

シークレット機能でOpenAI APIのAPIキーを設定し、以下2つのコードを実行しましょう。
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""### 【データの用意】

サイドバーのファイルエリア上で「data」フォルダを作り、フォルダ内に以下3つのファイルをアップロードしましょう。  
※まずはGoogleドライブからダウンロードする必要があります。

**ファイルが保存されているフォルダ**

「LangChainの主要モジュール4【Retrieval（RAG）】」フォルダ  
└「演習問題/提出課題」フォルダ  
└└「提出課題用データ」フォルダ

**ファイル**

*   一億総活躍社会の実現に向けて.pdf
*   雇用・失業調整の動向.pdf
*   従業員からの介護相談の対応について.docx

## 提出課題

### 【問題文】

「data」フォルダ内の全ファイルを外部参照先として、ユーザーからの質問に回答するRAGシステムを作りましょう。

**【条件】**
*   1回目の入力内容は「働き方改革の基本的な考え方を教えてください。」とします。
*   会話履歴の記憶機能を実装してください。
*   2回目の入力内容を「回答を一言で要約してください。」とし、会話履歴を踏まえてLLMが回答してくれることを確認しましょう。
*   すでにデータベースが存在する場合は読み込み、存在しない場合は新規作成して保存しましょう。
*   データベース名は「.db」とします。
*   ドキュメント分割には「CharacterTextSplitter」を使い、分割サイズは1000文字、  
前後の文字の重なりは50文字、分割のための区切り文字は「\n」としましょう。
*   「os.listdir(フォルダ名)」を使い、フォルダ内の各ファイル名を要素に持つリストを取得した後、各ファイルに対してfor文で順にドキュメントを読み込みましょう。

**【注記】**  
RAG化するファイルについて、拡張子「.pdf」のファイルが2つ、「.docx」のファイルが1つ存在します。「.pdf」のファイル読み込みには教材で学んだ通り「PyMuPDFLoader」を使い、「.docx」のファイル読み込みには「Docx2txtLoader」を使いましょう。このdocument loaderを使うにあたっては、「docx2txt」パッケージをインストールする必要があります。なお、バージョンは「0.8」としてください。使い方は「PyMuPDFLoader」と同じです。

### 【ヒント】

os.listdir()で「data」フォルダ内の各ファイル名を要素に持つリストを取得し、for文で順にdocument loaderで読み込む際、ファイル名の末尾が「.pdf」であれば「PyMuPDFLoader」、「.docx」であれば「Docx2txtLoader」を使うよう分岐処理を行うことで、拡張子が異なるファイル一式を読み込めます。

### 【解答】
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install chromadb==0.5.11
# %pip install -qU langchain-community beautifulsoup4
# %pip install tiktoken==0.8.0
# %pip install pymupdf==1.24.11
# %pip install docx2txt==0.8

# --------------------------------------------------
# 1.Document loaders
# --------------------------------------------------

# angchain_communityパッケージのdocument_loadersモジュールから、
# PyMuPDFLoaderクラス、Docx2txtLoaderクラスを読込む。
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders import Docx2txtLoader

import os

pages = []

abs_path = os.path.abspath('.')
dir_name = 'data'
path = os.path.join(abs_path, dir_name)
# フォルダ内の各ファイル名を要素に持つリストを取得
doc_list = os.listdir("/content/data")
# 各ファイルに対してfor文で順にドキュメントを読み込み
for indx, doc in enumerate(doc_list):
  print(doc)
  doc_paths = os.path.join(path, doc)
  if doc.lower().endswith(".pdf"):
    # PyMuPDFLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
    loader = PyMuPDFLoader(doc_paths)
  else:
    loader = Docx2txtLoader(doc_paths)

  # 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
  page = loader.load()

  # リスト同士を結合
  pages.extend(page)

  # len関数でリストの要素数を確認
  print(pages[indx].page_content)

# --------------------------------------------------
# 2.Document transformers
# --------------------------------------------------
# ext_splitterモジュールからCharacterTextSplitterクラスを読み込み、インスタンス化
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 分割後の各チャンク（テキストの塊）の最大文字数。このchunk_sizeの値を超えない範囲で分割が行われる。
    chunk_size=1000,
    # 分割後の各チャンクの文脈を保持するために指定する、各チャンクにまたがる前後の文字数。
    chunk_overlap=50,
    # チャンクを分割する区切り文字。chunk_sizeの範囲内で文字数が一番大きくなるよう区切られる。
    separator="\n",
)
# CharacterTextSplitterのインスタンスに対して「split_documents()」メソッドを呼び出し、
# 引数に読み込んだドキュメント（リストの各要素がPDFファイルの1ページ分のテキストであるもの）を指定
splitted_pages = text_splitter.split_documents(pages)

len(splitted_pages)

# --------------------------------------------------
# 3.Text embedding models
# --------------------------------------------------
# OpenAIEmbeddingsクラスを読み込み、インスタンスを作る
from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# --------------------------------------------------
# 4.Vector Stores
# --------------------------------------------------
# 「vectorstores」モジュールからChroma（クロマ）クラスを読み込む
from langchain.vectorstores import Chroma
# 「from_documents()」メソッドを使ってインスタンス化
# メソッドの引数には以下2つのデータを渡している。
# 1.Documentオブジェクトを要素に持つリスト（変数「splitted_pages」）
# 2.テキストをベクトル化するためのOpenAIEmbeddingsクラスのインスタンス（変数「embeddings」）
# コードを実行することで、内部的にドキュメント内のテキストがベクトル化され、
# ベクトル化されたテキストがChromaのデータベースに保存される。
# 実行ファイルと同じフォルダ内に「.db」という名前のフォルダが作成される。
if os.path.isdir(".db"):
    db = Chroma(persist_directory=".db", embedding_function=embeddings)
    # 新しい文書を足すときは add_documents を使う
    db.add_documents(splitted_pages)
    db.persist()
else:
    db = Chroma.from_documents(splitted_pages, embedding=embeddings, persist_directory=".db")
    db.persist()

retriever = db.as_retriever()

# -------------------------------------------------------------
# 「外部情報の参照」と「会話履歴の記憶」を同時に実現する仕組み
# 1.LLMに最新の入力内容と会話履歴を渡し、会話履歴なしでも理解できるLLMへの独立した（新しい）入力内容を生成する（1回目のLLMリクエスト）。
# 2.生成された入力内容をもとに、ベクターストアから関連するドキュメントを取得する。
# 3.生成された入力内容と、ベクターストアから取得した関連ドキュメントをプロンプトに埋め込み、LLMに渡して回答を生成する（2回目のLLMリクエスト）。
# 4.Chains内ではLLMへのリクエストが2回行われます。特に注目するべきは、1回目のリクエストです。
# -------------------------------------------------------------

# これらの処理を実現するために、以下3つのインスタンスを順に用意します。
# 1.会話履歴なしでも理解できる独立した入力内容を生成し、これをもとにベクターストアから適切な関連ドキュメントを取得する「create_history_aware_retriever」のインスタンス（手順1と2に対応）
# 2.生成した入力内容と取得した関連ドキュメントをもとに、LLMに回答を生成させる「create_stuff_documents_chain」のインスタンス（手順3に対応）
# 3.これら2つのインスタンスをまとめて、手順1〜3を連鎖的に実行する機能を持つ「create_retrieval_chain」のインスタンス

# ***********************************************************
# 1. create_history_aware_retriever（手順1と2に対応）
# ***********************************************************
# 会話履歴の記憶機能を実装するにあたって必要なクラスや関数などを一式読み込んでおく。
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
# 「create_history_aware_retriever」は、「手順1: 独立した入力内容の生成」と
# 「手順2: 生成した入力内容をもとにドキュメントを取得」を行える。
# 「create_retrieval_chain」は、「create_history_aware_retriever」と
# 「create_stuff_documents_chain」をまとめるChain。
# 最終的にこのクラスの機能を使うことで、手順1〜3の処理を実行
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
#「create_stuff_documents_chain」は、「手順3: 独立した入力内容と
# 関連ドキュメントをもとに回答を生成」を行える。
from langchain.chains.combine_documents import create_stuff_documents_chain

# 手順1〜3の処理を実現するにあたり、LLMへのリクエストは以下の2回行われる。
# 1.会話履歴がなくても理解できる、独立した入力を生成するためのLLMリクエスト
# 2.生成された入力内容と関連ドキュメントを渡して、最終的な回答を生成するためのLLMリクエスト
# ここでは「1. 会話履歴がなくても理解できる、独立した入力を生成するためのLLMリクエスト」を行うための、専用のプロンプトを用意。
question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"

# ChatPromptTemplateでは、LLMの振る舞いを制御するシステムメッセージとユーザーメッセージ、
# また会話履歴を差し込むためのプレースホルダーを用意している。
# システムメッセージとユーザーメッセージは、このように省略した書き方が可能。
question_generator_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_generator_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),

    ]
)
# 呼び出すLLMのインスタンスを用意。
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# 呼び出すLLMと、ベクターストア検索のためのRetriever、
# また独立した入力生成用のプロンプトを渡すことで
# 「create_history_aware_retriever」のインスタンスを生成。
# Retrieverには、「Retrievers」の前パートで作成したインスタンス
# (retriever = db.as_retriever())を使う。
# これで、手順1と2を実行する準備が完了。
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, question_generator_prompt
)

# ***********************************************************
# 2. create_stuff_documents_chain（手順3に対応）
# ***********************************************************
# 会話履歴なしでも理解できる独立した入力内容と、
# ベクターストアから取得した関連ドキュメントをもとに
# LLMから回答を得るためのプロンプトを用意。
# 「{context}」の箇所に関連ドキュメントが埋め込まれる。
# このプロンプトを使うことで、入力内容に対して会話履歴を踏まえた回答を得られる。
question_answer_template = """
あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"
{context}
"""
question_answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", question_answer_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 呼び出すLLMとプロンプトを引数として渡し
# 「create_stuff_documents_chain」のインスタンスを生成。
# このインスタンスの機能を使うことで、会話履歴なしでも理解できる
# 独立した入力内容と取得した関連ドキュメントをもとに、LLMに回答を生成させることができる。
question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)

# ***********************************************************
# 3. create_retrieval_chain（1と2のインスタンスをまとめたChains（手順1〜3に対応））
# ***********************************************************
# 1つ目に作った「create_history_aware_retriever」のインスタンスを使うことで、
# 会話履歴なしでも理解できる独立した入力内容の生成と、適した関連ドキュメントの取得を行える。
# そして2つ目に作った「create_stuff_documents_chain」のインスタンスを使うことで、
# 最終的にLLMから回答を得られる。
# これら2つのインスタンスの機能をまとめたChainsが、「create_retrieval_chain」。
# このChainsを実行することで、「外部情報の参照」と「会話履歴の記憶」を
# 同時に実現するための一連の機能を連鎖的に実行できる。

# 引数には、先ほど作成した「create_history_aware_retriever」のインスタンスと、
# 「create_stuff_documents_chain」のインスタンスを渡す。
# 後ほど、この「create_retrieval_chain」のインスタンスが持つ「invoke()」メソッドに
# 「入力内容」と「会話履歴」の2つのデータを渡すことで、独立した入力内容の生成と
# 関連ドキュメントの取得、最終的なLLMからの回答生成を内部的に一括で行える。
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# 「invoke()」メソッドを実行してLLMに回答を生成させる前に、以下のコードを実行しておく。
# 「openai」ライブラリのログレベルを「DEBUG」に設定している。
# これによりopenaiライブラリが使われた際の内部動作が、
# プログラムの実行結果として詳細に表示されるようになる。
import logging
logging.getLogger("openai").setLevel(logging.DEBUG)

# LLM呼び出しを行う前に、会話履歴を保持するためのデータの入れ物を用意。
# 2回目以降のLLM呼び出しでは、入力内容と会話履歴をもとに、
# 会話履歴なしでもLLMが理解できる「独立した入力内容」を生成する。
# そのため入力内容とLLMからの回答内容は、LLM呼び出しのたびに
# 会話履歴として保存していく必要がある。
chat_history = []

# 入力内容を変数「query」として用意した後、
# create_retrieval_chainのインスタンス「rag_chain」に対して
# 「invoke()」メソッドを実行している。引数には入力内容と会話履歴を渡す。
query = "働き方改革の基本的な考え方を教えてください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})

print(ai_msg["answer"])

# 「extend()」は、リスト同士を結合するメソッド。
# 入力内容とLLMからの回答内容を要素に持つリストを渡すことで、会話履歴が更新される。
chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])

query = "回答を一言で要約してください。"
ai_msg = rag_chain.invoke({"input": query, "chat_history": chat_history})

print(ai_msg["answer"])

chat_history.extend([HumanMessage(content=query), ai_msg["answer"]])