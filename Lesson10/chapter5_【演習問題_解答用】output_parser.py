# -*- coding: utf-8 -*-
"""Chapter5:【演習問題: 解答用】Output Parser

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VivIFliKzDOWJUXu_67qfN2gcdR05XXk

# Lesson10: LangChainの主要モジュール1【Model I/O】：Output Parser
# Chapter5:【演習問題: 解答用】Output Parser

事前準備を行った上で、3つの演習問題に取り組みましょう。

各問題の「回答例/正解」と「解説」はデフォルトで非表示としていますが、  
非表示セルをクリックすれば確認できます。

まずは「回答例/正解」と「解説」を見ずにトライしてみましょう。

## 事前準備

OpenAI APIキーの設定
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

"""各種パッケージのインストール"""

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 langchain-openai==0.2.2 httpx==0.27.2

"""## 演習問題

### 【問題1】
以下の条件で、異なる色のフルーツをLLMに3つ生成させ、リスト形式に変換したものを出力しましょう。  

**【条件】**


*   色は、ユーザー入力で受け取りましょう。
*   リストに変換しやすい形式でLLMからの回答を得られるよう、  
型変換のOutput Parserで生成したフォーマット文字列をプロンプトに埋め込みましょう。
"""

# ------------------------------------------------------------
# 1. ライブラリを読み込む
# ------------------------------------------------------------
# 一行目でoutput_parsersモジュールから「CommaSeparatedListOutputParser」クラスを読み込んでいる。このクラスを使うことで、LLMからリスト形式で型変換しやすいように回答を生成させることができる。
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ------------------------------------------------------------
# 2. Output Parserを用意する
# ------------------------------------------------------------
# 読み込んだCommaSeparatedListOutputParserクラスのインスタンスを作る
output_parser = CommaSeparatedListOutputParser()

# ------------------------------------------------------------
# 3. プロンプトに追加するフォーマット命令を作る
# ------------------------------------------------------------
# Output Parserの「get_format_instructions()」メソッドを呼び出すことで、プロンプトに含める「データ形式を指定する命令（フォーマット命令）」を生成できる。
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# ------------------------------------------------------------
# 4. フォーマット命令を含めたプロンプトを作る
# ------------------------------------------------------------
# フォーマット命令がテキストで格納された「format_instruction」を使い、PromptTemplateを作る
# テンプレートに「partial_variables」でフォーマット命令を追加した上で、PromptTemplateを作っている
prompt_template = PromptTemplate(
    template=" {color}と異なる色のフルーツを3つ挙げてください。\n{format_instruction}",
    input_variables=["color"],
    partial_variables={"format_instruction": format_instruction}
)

# どのようなプロンプトがLLMに渡されるかを確認するため、PromptTemplateのformat_prompt()メソッドでプロンプトのテキストを表示
color = input("フルーツの色を入力してください：")
prompt = prompt_template.format_prompt(color=color)
print(prompt.text)

# ------------------------------------------------------------
# 5. LLMにプロンプトを渡し、出力を得る
# ------------------------------------------------------------
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
# HumanMessageにプロンプトのテキストを設定し、LLMにプロンプトを渡して出力
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

# LLMからのレスポンス「result」の「content」フィールドに、LLMからの回答テキストが格納されている
output = llm(messages)
print(output.content)

# ------------------------------------------------------------
# 6. 出力を構文解析し、プログラムで扱いやすいデータ形式に変換する
# ------------------------------------------------------------
# Output Parserに対して「parse()」メソッドを実行し、引数にLLMからの出力テキストを渡すことで、以下のようにリスト形式のデータに変換
result = output_parser.parse(output.content)
print(result)

"""#### 【解答例】"""

# 各種クラスの読み込み
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 出力形式をリストで指定するためのOutput Parsermのインスタンスを作成
output_parser = CommaSeparatedListOutputParser()

# プロンプトに埋め込むフォーマット文字列を作成
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# ユーザー入力値を受け取り、フォーマット文字列を含むプロンプトを生成
template = """
次のフォーマットで、与えられた色のフルーツを3つ生成してください。

{format_instruction}

フルーツの色: {color}
"""

prompt_template = PromptTemplate(
    template=template,
    input_variables=["color"],
    partial_variables={"format_instruction": format_instruction}
)

input_color = input("フルーツの色を入力してください: ")
prompt = prompt_template.format_prompt(color=input_color)

# LLMにプロンプトを渡し、回答を生成
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

output = llm(messages)
print(output.content)

# LLMからの回答をリスト形式に変換
result = output_parser.parse(output.content)
print(result)

"""#### 【解説】

リスト形式に変換しやすい回答をLLMから得るために使うのは「CommaSeparatedListOutputParser」クラスです。

フォーマット文字列とユーザー入力値を埋め込んだプロンプトを生成し、それをLLmに渡すことでリスト形式に変換しやすい回答を得られます。

最後にパース処理（型変換の処理）を行い、プログラム上で扱いやすいようリスト形式に変換しています。

### 【問題2】
Enumクラスを継承したクラスの中にLLMからの回答の選択肢を5つ用意し、  
初めての海外旅行におすすめの国を選択肢の名からLLMに1つ回答させ、  
Enumクラスを継承したクラスの属性データに変換しましょう。
"""

# ------------------------------------------------------------
# 1. ライブラリを読み込む
# ------------------------------------------------------------
# 一行目でoutput_parsersモジュールから「EnumOutputParser」クラスを読み込んでいる
from langchain.output_parsers.enum import EnumOutputParser
from enum import Enum
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ------------------------------------------------------------
# 2. LLMから返してほしい回答の型を定義する
# ------------------------------------------------------------
# ここでは「観光におすすめの都道府県を1つ挙げてください。」というプロンプトに対して、用意した定数のうちいずれかをLLMに選んでもらう想定で、Enumクラスを継承した「Prefectures」クラスを定義
class Prefectures(Enum):
    PREFECTURES_1 = "ハワイ"
    PREFECTURES_2 = "韓国"
    PREFECTURES_3 = "グアム"
    PREFECTURES_4 = "サイパン"
    PREFECTURES_5 = "バリ島"

# ------------------------------------------------------------
# 3. Output Parserを用意する
# ------------------------------------------------------------
# 読み込んだEnumOutputParserクラスのインスタンスを作り、enum引数に定義したEnumオブジェクト（Prefecturesクラス）を渡す
output_parser = EnumOutputParser(enum=Prefectures)

# ------------------------------------------------------------
# 4. 定義した回答の型に則って、プロンプトに追加するフォーマット命令を作る
# ------------------------------------------------------------
# Output Parserのget_format_instructions()メソッドを呼び出すことで、プロンプトに含める「データ形式を指定する命令（フォーマット命令）」を生成
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# ------------------------------------------------------------
# 5. フォーマット命令を含めたプロンプトを作る
# ------------------------------------------------------------
# フォーマット命令がテキストで格納された「format_instruction」を使い、PromptTemplateを作る
prompt_template = PromptTemplate(
    # 選択肢として用意したものと合致する回答でないと、後ほどEnumクラスのデータへの変換時にエラーが発生するため、「ただし、回答は都道府県名のみとしてください。」という制約を加えている
    template="初めての海外旅行におすすめの国を1つ挙げてください。ただし、回答は国名のみとしてください。\n{format_instruction}",
    input_variables=[],
    # テンプレートに「partial_variables」でフォーマット命令を追加した上で、PromptTemplateを作る。
    partial_variables={"format_instruction": format_instruction}
)
# どのようなプロンプトがLLMに渡されるかを確認するため、PromptTemplateのformat_prompt()メソッドでプロンプトのテキストを表示
prompt = prompt_template.format_prompt()
print(prompt.text)

# ------------------------------------------------------------
# 6. LLMにプロンプトを渡し、出力を得る
# ------------------------------------------------------------
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

output = llm(messages)
print(output.content)

# ------------------------------------------------------------
# 7. 出力を構文解析し、プログラムで扱いやすいデータ形式に変換する
# ------------------------------------------------------------
# Output Parserに対して「parse()」メソッドを実行し、引数にLLMからの出力テキストを渡す
result = output_parser.parse(output.content)
print(result)
# Enumクラスのデータからテキストを取り出す際は、以下のように「value」属性を参照
print(result.value)

"""#### 【解答例】"""

# 各種クラスの読み込み
from langchain.output_parsers.enum import EnumOutputParser
from enum import Enum
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 回答の選択肢をEnumクラスを継承したクラスで用意
class Countrys(Enum):
    COUNTRY_1 = "韓国"
    COUNTRY_2 = "フランス"
    COUNTRY_3 = "イタリア"
    COUNTRY_4 = "オーストラリア"
    COUNTRY_5 = "シンガポール"

# 用意した選択肢の中からLLMに回答させるため、EnumOutputParserのインスタンスを作成
output_parser = EnumOutputParser(enum=Countrys)

# プロンプトに埋め込むフォーマット文字列を作成
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# フォーマット文字列を含むプロンプトを生成
prompt_template = PromptTemplate(
    template="初めての海外旅行におすすめの国を1つ挙げてください。ただし、回答は国名のみとしてください。\n{format_instruction}",
    input_variables=[],
    partial_variables={"format_instruction": format_instruction}
)

prompt = prompt_template.format_prompt()
print(prompt.text)

# LLMにプロンプトを渡し、回答を生成
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

output = llm(messages)
print(output.content)

# LLMからの回答を、Enumクラスの属性データに変換
result = output_parser.parse(output.content)
print(result)

"""#### 【解説】

用意した選択肢の中からLLMに回答させたい場合に使うのは、Enum Parserです。

Enumクラスを継承したクラスを作り、各属性値に回答の選択肢を用意します。

Output Parserを作った後のコードは、【問題1】で解説したList Parserのコードとほとんど同じです。

### 【問題3】
以下のデータセットを使い、「新しい趣味を始めたい人におすすめの趣味を教えてください。」という命令に対して、辞書型に変換しやすい形式でLLMに回答してもらうためのフォーマット命令と合わせてプロンプトを作り、LLMからの回答を辞書型に変換して表示しましょう。

**【データセット】**


*   reason_for_recommendation: その趣味が魅力的でおすすめできるポイント
*   how_to_enjoy: 趣味を楽しむための、具体的な方法やアイデア
*   required_tools_and_preparation: その趣味を始めるために必要な道具や準備
*   benefits_of_the_hobby: その趣味を続けることで得られる心身や生活におけるメリット
*   places_to_start: その趣味を楽しむために適した場所や環境
*   precautions: 趣味を楽しむ際に気をつけるべきことや注意点
*   tips_for_continuation: その趣味を長く続けるためのモチベーションの保ち方や工夫
"""

# ------------------------------------------------------------
# 1. ライブラリを読み込む
# ------------------------------------------------------------
# output_parsersモジュールから「StructuredOutputParser」クラスと「ResponseSchema」クラスを読込む
# これはStructuredOutputParserで後ほど、LLMから返してほしい応答の型を定義するために使う
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# ------------------------------------------------------------
# 2. LLMから返してほしい回答の型を定義する
# ------------------------------------------------------------
# 「AI開発におすすめのプログラミング言語を3つ挙げてください」と質問し、それに対してJSON形式で回答を出力してもらうための制約となるコード
# 各キーには「description」を指定。これは各キーの説明であり、descriptionにキーの意味を正確に記載する
# 回答の型定義には先ほど読み込んだ「ResponseSchema」を使うが、これはLLMからの回答のデータ形式を定義するための、Output parserの特別なオブジェクト
schemas = [
    ResponseSchema(name="reason_for_recommendation", description="その趣味が魅力的でおすすめできるポイント"),
    ResponseSchema(name="how_to_enjoy", description="趣味を楽しむための、具体的な方法やアイデア"),
    ResponseSchema(name="required_tools_and_preparation", description="その趣味を始めるために必要な道具や準備"),
    ResponseSchema(name="benefits_of_the_hobby", description="その趣味を続けることで得られる心身や生活におけるメリット"),
    ResponseSchema(name="places_to_start", description="その趣味を楽しむために適した場所や環境"),
    ResponseSchema(name="precautions", description="趣味を楽しむ際に気をつけるべきことや注意点"),
    ResponseSchema(name="tips_for_continuation", description="その趣味を長く続けるためのモチベーションの保ち方や工夫")
]

# ------------------------------------------------------------
# 3. Output Parserを用意する
# ------------------------------------------------------------
# 回答の型を定義したら、その定義を使ってStructuredOutputParserクラスのインスタンスを作る
# StructuredOutputParserのfrom_response_schemas()メソッドを使い、引数にLLMからの回答の型を渡す
output_parser = StructuredOutputParser.from_response_schemas(schemas)

# ------------------------------------------------------------
# 4. 定義した回答の型に則って、プロンプトに追加するフォーマット命令を作る
# ------------------------------------------------------------
# Output Parserの「get_format_instructions()」メソッドを呼び出して、プロンプトに含める「データ形式を指定する命令（フォーマット命令）」を生成
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# ------------------------------------------------------------
# 5. フォーマット命令を含めたプロンプトを作る
# ------------------------------------------------------------
# フォーマット命令がテキストで格納された「format_instruction」を使い、PromptTemplateを作る
prompt_template = PromptTemplate(
    template="新しい趣味を始めたい人におすすめの趣味を教えてください。\n{format_instruction}",
    # テンプレート内の変数を、LLM呼び出しの際に渡した引数で置換するための「input_variables」は空
    input_variables=[],
    # その代わりに、「partial_variables」パラメータを指定
    # このパラメータは、LLM呼び出し時ではなく、PromptTemplateオブジェクトの作成時にテンプレート文字列内の変数を置換するもの
    # 「format_instruction」という名前の変数がテンプレート文字列に含まれていますが、これを先ほど用意した出力形式指定のためのフォーマット命令で置換
    partial_variables={"format_instruction": format_instruction}
)

# どのようなプロンプトがLLMに渡されるかを確認するため、PromptTemplateの「format_prompt()」メソッドでプロンプトのテキストを表示
prompt = prompt_template.format_prompt()
print(prompt.text)

# ------------------------------------------------------------
# 6. LLMにプロンプトを渡し、出力を得る
# ------------------------------------------------------------
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

# HumanMessageに prompt.text でプロンプトのテキストを設定
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

# LLMにプロンプトを渡して出力
# LLMからのレスポンス「result」の「content」フィールドに、LLMからの回答テキストが格納
output = llm(messages)
print(output.content)

# ------------------------------------------------------------
# 7. 出力を構文解析し、プログラムで扱いやすいデータ形式に変換する
# ------------------------------------------------------------
# Output Parserに対して「parse()」メソッドを実行し、引数にLLMからの出力テキストを渡し、プログラムで扱いやすいデータ型（ここでは辞書型）に変換
result = output_parser.parse(output.content)
print(result)

"""#### 【解答例】"""

# 各種クラスの読み込み
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# LLMから回答として受け取るデータ構造の用意
schemas = [
    ResponseSchema(name="hobby_name", description="その趣味の具体的な名前を示します。"),
    ResponseSchema(name="reason_for_recommendation", description="その趣味が魅力的でおすすめできるポイントを説明します。"),
    ResponseSchema(name="how_to_enjoy", description="趣味をどのように楽しむか、具体的な方法やアイデアを提示します。"),
    ResponseSchema(name="required_tools_and_preparation", description="その趣味を始めるために必要な道具や準備について説明します。"),
    ResponseSchema(name="benefits_of_the_hobby", description="その趣味を続けることで得られる心身や生活のメリットを説明します。"),
    ResponseSchema(name="places_to_start", description="その趣味を楽しむために適した場所や環境について説明します。"),
    ResponseSchema(name="precautions", description="趣味を楽しむ際に気をつけるべきことや注意点について説明します。"),
    ResponseSchema(name="tips_for_continuation", description="その趣味を長く続けるためのモチベーションの保ち方や工夫について説明します。")
]

# 定義した構造化データに沿った形式でLLMから回答を得るためにStructuredOutputParserのインスタンスを作成
output_parser = StructuredOutputParser.from_response_schemas(schemas)

# プロンプトに埋め込むフォーマット文字列を作成
format_instruction = output_parser.get_format_instructions()
print(format_instruction)

# フォーマット文字列を含むプロンプトを生成
prompt_template = PromptTemplate(
    template="新しい趣味を始めたい人におすすめの趣味を教えてください。\n{format_instruction}",
    input_variables=[],
    partial_variables={"format_instruction": format_instruction}
)

prompt = prompt_template.format_prompt()
print(prompt.text)

# LLMにプロンプトを渡し、回答を生成
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content=prompt.text),
]

output = llm(messages)
print(output.content)

# LLMからの回答を、用意したデータ構造に沿った形式で辞書型に変換
result = output_parser.parse(output.content)
result

"""#### 【解説】

LLMの回答結果を辞書型に変換するために、「StructuredOutputParser」を使います。

辞書のキーとバリューの形式でLLMに回答を生成してもらえるよう、  
キー名に該当する「name」と、バリューに該当する「キーの説明」をResponseSchemaを使って用意し、  
生成したフォーマット命令をプロンプトに埋め込んで回答を生成しています。
"""