# -*- coding: utf-8 -*-
"""Chapter11:【提出課題: 解答用】「Agents」×「Retrieval」の本格的な生成AIシステムを開発してみよう

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18-0KTcTdKwVrrlfzMt_wcNks1Y5VZTPM

# Lesson14: LangChainの主要モジュール5【Agents（AIエージェント）】
# Chapter11:【提出課題: 解答用】「Agents」×「Retrieval」の本格的な生成AIシステムを開発してみよう

事前準備を行った上で、提出課題に取り組みましょう。

**【提出方法】**  
Slackでメンターをメンションの上、当シート右上の「共有」からリンクをコピーし、提出してください。

## 事前準備

### 【OpenAI APIキーの設定】

シークレット機能でOpenAI APIのAPIキーを設定し、以下2つのコードを実行しましょう。
"""

import os
from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

!pip install langchain==0.3.0 openai==1.47.0 langchain-community==0.3.0 httpx==0.27.2 pydantic==2.9.2

"""### 【データの用意】

サイドバーのファイルエリア上で「data1」フォルダと「data2」フォルダを作り、  
それぞれフォルダ内に以下のファイルをアップロードしましょう。  
※まずはGoogleドライブからダウンロードする必要があります。

**「data1」用のファイルが保存されているフォルダ**

「Lesson14: LangChainの主要モジュール5【Agents（AIエージェント）】」フォルダ  
└「演習問題/提出課題」フォルダ  
└└「data1」フォルダ

**「data2」用のファイルが保存されているフォルダ**

「Lesson14: LangChainの主要モジュール5【Agents（AIエージェント）】」フォルダ  
└「演習問題/提出課題」フォルダ  
└└「data2」フォルダ

## 提出課題

### 【問題文】

以下の条件に従い、「Agents」×「Retrieval」の本格的な生成AIシステムを開発しましょう。

**【条件】**
*   自社サービスhealthXに関するデータが格納されている「data1」フォルダ内のファイルをRAG化し、1つ目のToolを作る。
*   雇用に関する一般的なデータが格納されている「data2」フォルダ内のファイルをRAG化し、2つ目のToolを作る。
*   作成した2つのToolをエージェントに渡し、ユーザー入力値に応じて適切なToolが選ばれるようにする。
*   ユーザー入力値は、あらかじめ用意されている2つを順に使ってエージェントを実行する。
*   RAGシステムについて、会話履歴の記憶機能を搭載する。

### 【ヒント】

*   指定フォルダ内のファイルを一式RAG化するコードは、Lesson13の「Chapter5: 演習問題」の問題5を参照してください。
*   「Agents」×「Retrieval」の基本的なコードは、当LessonのChapter8を参照してください。
*   「data2」フォルダ内に格納されている拡張子「.docx」のファイルを読み込むために適したdocument loadersを、専用のパッケージをインストールの上で使ってください。

### 【解答】
"""

query_1 = "HealthXの特徴を教えてください"
query_2 = "働き方改革の基本的な考え方を教えてください"

from google.colab import drive
drive.mount('/content/drive')

!pip install pymupdf==1.24.11 tiktoken==0.8.0
!pip install docx2txt==0.8

# 必要なクラスや関数を一式読み込む
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, AIMessage
from langchain import LLMChain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import Docx2txtLoader

# ファイルパスを引数として受け取り、
# そのファイル内の情報を読み込んでRAG化し、
# 会話履歴の記憶機能を搭載したChainsを作成する用の関数を定義

def create_rag_chain(path):
  pages = []

  #abs_path = os.path.abspath('.')
  #dir_name = 'data'
  #path = os.path.join(abs_path, dir_name)
  #print(path)
  # フォルダ内の各ファイル名を要素に持つリストを取得
  doc_list = os.listdir(path)

  # 各ファイルに対してfor文で順にドキュメントを読み込み
  for indx, doc in enumerate(doc_list):
    doc_paths = os.path.join(path, doc)
    if doc.lower().endswith(".pdf"):
      # PyMuPDFLoaderクラスをインスタンス化(引数に読み込みたいファイル名を指定)
      loader = PyMuPDFLoader(doc_paths)
    else:
      loader = Docx2txtLoader(doc_paths)

    # 生成したPyMuPDFLoaderのインスタンスに対して「load()」メソッドを実行
    page = loader.load()

    # リスト同士を結合
    pages.extend(page)

  # len関数でリストの要素数を確認
  len(pages)

  text_splitter = CharacterTextSplitter(
      chunk_size=500,
      chunk_overlap=30,
      separator="\n",
  )

  splitted_pages = text_splitter.split_documents(pages)
  embeddings = OpenAIEmbeddings()

  db = Chroma.from_documents(splitted_pages, embedding=embeddings)
  retriever = db.as_retriever()

  question_generator_template = "会話履歴と最新の入力をもとに、会話履歴なしでも理解できる独立した入力テキストを生成してください。"
  question_generator_prompt = ChatPromptTemplate.from_messages(
      [
          ("system", question_generator_template),
          MessagesPlaceholder("chat_history"),
          ("human", "{input}"),
      ]
  )

  question_answer_template = """
  あなたは優秀な質問応答アシスタントです。以下のcontextを使用して質問に答えてください。
  また答えが分からない場合は、無理に答えようとせず「分からない」という旨を答えてください。"
  {context}
  """

  question_answer_prompt = ChatPromptTemplate.from_messages(
      [
          ("system", question_answer_template),
          MessagesPlaceholder("chat_history"),
          ("human", "{input}"),
      ]
  )

  llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
  history_aware_retriever = create_history_aware_retriever(
      llm, retriever, question_generator_prompt
  )

  question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
  rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
  return rag_chain

# ファイル別にChainsを作成
data1_chain = create_rag_chain("/content/data1")
data2_chain = create_rag_chain("/content/data2")

# Chainsについて、
# それぞれ会話履歴の記憶機能を実装するため、
# Chainsごとに空のリストを用意
data1_chain_chat_history = []
data2_chain_chat_history = []

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化したChains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_data1_doc_chain(param):
  # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
  ai_msg = data1_chain.invoke({"input": param, "chat_history": data1_chain_chat_history})
  # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
  data1_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
  return ai_msg["answer"]

# エージェントが「このToolを使うべき」と判断した際、
# この関数にユーザー入力値が渡されてToolが実行される。この関数こそToolの実体。
# ユーザー入力値を引数「param」で受け取り、data1フォルダに格納されたファイルをRAG化したChains「data1_chain」にユーザー入力値と会話履歴を渡して実行
def run_data2_doc_chain(param):
  # LLMが生成した回答が変数「ai_msg」に格納され、それを先ほどグローバル変数として定義した変数「data1_chain_chat_history」に追加。
  ai_msg = data2_chain.invoke({"input": param, "chat_history": data2_chain_chat_history})
  # LLMに会話履歴の内容を理解してもらいやすくするため、追加の際はHumanMessageとAIMessageのそれぞれのオブジェクトとして追加
  data2_chain_chat_history.extend([HumanMessage(content=param), AIMessage(content=ai_msg["answer"])])
  return ai_msg["answer"]

# data1_doc_tool作成
# 引数「func」に先ほど定義した関数を渡し、エージェントがToolを適切に選択できるよう、nameとdescriptionを指定。
from langchain.tools import Tool
data1_doc_tool = Tool.from_function(
    func=run_data1_doc_chain,
    name="自社サービス「HealthX」に関する情報を参照するTool",
    description="自社サービス「HealthX」に関する情報を参照したい場合に使う（例：「HealthXの特徴」、「HealthXの料金」）"
)

# data2_doc_tool作成
# 引数「func」に先ほど定義した関数を渡し、エージェントがToolを適切に選択できるよう、nameとdescriptionを指定。
from langchain.tools import Tool
data2_doc_tool = Tool.from_function(
    func=run_data2_doc_chain,
    name="自社の「多様な人々が安心して働き続けられる社会実現への取り組み」に関する情報を参照するTool",
    description="自社自社の「多様な人々が安心して働き続けられる社会実現への取り組み」に関する情報を参照したい場合に使う（例：「従業員からの介護相談の対応」、「雇用・失業調整の動向」、「働き方改革」）"
)

# Agent Executor作成
from langchain.agents import AgentType, initialize_agent, load_tools

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

tools = [data1_doc_tool, data2_doc_tool]

agent_executor = initialize_agent(
    llm=llm,
    tools=tools,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

agent_executor.run(query_1)

agent_executor.run(query_2)